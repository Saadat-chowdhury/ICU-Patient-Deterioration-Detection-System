{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07ace10a-b386-42d5-a4f8-c667ba0c3a23",
   "metadata": {},
   "source": [
    "# ICU Patient Deterioration Detection - Data Exploration & Feature Extraction\n",
    "\n",
    "**Project Goal:** Build an explainable early warning system that predicts ICU patient deterioration using machine learning and generates clinical explanations using LLMs.\n",
    "\n",
    "**This Notebook:** \n",
    "- Explores MIMIC-III waveform database\n",
    "- Filters for usable segments (>= 4 hours with ECG + BP)\n",
    "- Extracts vital sign features for ML training\n",
    "\n",
    "**Dataset:** MIMIC-III Waveform Database (adult ICU patients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a992cac-8984-4611-86ad-9e1c9d030d64",
   "metadata": {},
   "source": [
    "## Step 1: Load Adult Patient List\n",
    "\n",
    "The RECORDS-adults file contains paths to all adult ICU patients in the database. We'll load this list to identify which patients we've downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e13d30c-a216-4c30-b83b-13d584c164d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total adult patients available: 59344\n",
      "You downloaded: 2273 adult patients\n",
      "\n",
      "First 10 you downloaded:\n",
      "30/3000003\n",
      "30/3000031\n",
      "30/3000060\n",
      "30/3000063\n",
      "30/3000065\n",
      "30/3000086\n",
      "30/3000100\n",
      "30/3000103\n",
      "30/3000105\n",
      "30/3000125\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Load adult patient list\n",
    "with open('RECORDS-adults', 'r') as f:\n",
    "    adult_patients = [line.strip() for line in f]\n",
    "\n",
    "# Check which ones you actually downloaded\n",
    "downloaded = []\n",
    "for patient in adult_patients:\n",
    "    # Remove trailing slash and split\n",
    "    patient_clean = patient.rstrip('/')\n",
    "    parts = patient_clean.split('/')\n",
    "    \n",
    "    if len(parts) == 2:\n",
    "        folder, patient_id = parts\n",
    "        if os.path.exists(f'data/{folder}/{patient_id}'):\n",
    "            downloaded.append(patient_clean)\n",
    "\n",
    "print(f\"Total adult patients available: {len(adult_patients)}\")\n",
    "print(f\"You downloaded: {len(downloaded)} adult patients\")\n",
    "print(f\"\\nFirst 10 you downloaded:\")\n",
    "for i in range(min(10, len(downloaded))):\n",
    "    print(downloaded[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c332c7cb-615c-4609-a07f-14fa91826f8e",
   "metadata": {},
   "source": [
    "## Step 2: Identify Downloaded Patients\n",
    "\n",
    "Check which adult patients from the RECORDS-adults list actually exist in our local data folder. We downloaded folders 30-39, giving us access to thousands of patients, but we'll work with a manageable subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2e6ef8c-d49e-4f0a-a108-083c497f7924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with: 30 patients\n",
      "\n",
      "Your working set:\n",
      "30/3000003\n",
      "30/3000031\n",
      "30/3000060\n",
      "30/3000063\n",
      "30/3000065\n",
      "30/3000086\n",
      "30/3000100\n",
      "30/3000103\n",
      "30/3000105\n",
      "30/3000125\n",
      "30/3000126\n",
      "30/3000142\n",
      "30/3000154\n",
      "30/3000189\n",
      "30/3000190\n",
      "30/3000203\n",
      "30/3000221\n",
      "30/3000282\n",
      "30/3000336\n",
      "30/3000393\n",
      "30/3000397\n",
      "30/3000428\n",
      "30/3000435\n",
      "30/3000458\n",
      "30/3000480\n",
      "30/3000484\n",
      "30/3000497\n",
      "30/3000531\n",
      "30/3000577\n",
      "30/3000596\n"
     ]
    }
   ],
   "source": [
    "# Let's pick the first 30 patients you downloaded\n",
    "working_patients = downloaded[:30]\n",
    "\n",
    "print(f\"Working with: {len(working_patients)} patients\")\n",
    "print(\"\\nYour working set:\")\n",
    "for p in working_patients:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28bf94a-4011-46e3-961a-df0d1cb75ee0",
   "metadata": {},
   "source": [
    "## Step 3: Select Working Set of Patients\n",
    "\n",
    "For this project, we'll work with 30 patients. This is manageable within our 4-week timeline while still providing sufficient data for ML training (~100-150 segments expected)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b087619d-db9e-4e64-b627-4177e497f104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3000003:\n",
      "  - 3000003_0001\n",
      "  - 3000003_0002\n",
      "  - 3000003_0003\n",
      "  - 3000003_0004\n",
      "  - 3000003_0005\n",
      "\n",
      "3000031:\n",
      "\n",
      "3000060:\n",
      "  - 3000060_0001\n",
      "  - 3000060_0002\n",
      "  - 3000060_0003\n",
      "  - 3000060_0004\n",
      "  - 3000060_0005\n",
      "\n",
      "3000063:\n",
      "  - 3000063_0001\n",
      "  - 3000063_0002\n",
      "  - 3000063_0003\n",
      "  - 3000063_0004\n",
      "  - 3000063_0005\n",
      "\n",
      "3000065:\n",
      "  - 3000065_0001\n",
      "  - 3000065_layout\n"
     ]
    }
   ],
   "source": [
    "import wfdb\n",
    "\n",
    "# Find all segments for your working patients\n",
    "all_segments = []\n",
    "\n",
    "for patient in working_patients[:5]:  # Start with just 5 to test\n",
    "    folder, patient_id = patient.split('/')\n",
    "    patient_path = f'data/{folder}/{patient_id}'\n",
    "    \n",
    "    # Find all segment files\n",
    "    files = os.listdir(patient_path)\n",
    "    segment_files = [f for f in files if f.endswith('.hea') and '_' in f]\n",
    "    \n",
    "    print(f\"\\n{patient_id}:\")\n",
    "    for seg_file in segment_files[:5]:  # Show first 5 segments\n",
    "        seg_name = seg_file.replace('.hea', '')\n",
    "        print(f\"  - {seg_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26b1d36-d12c-49d7-a016-c132a2ab133e",
   "metadata": {},
   "source": [
    "## Step 4: Explore Segments and Filter for Usability\n",
    "\n",
    "Each patient has multiple recording segments representing different time periods during their ICU stay.\n",
    "\n",
    "**What we're checking:**\n",
    "- Which segments exist for each patient\n",
    "- Segment duration (need >= 4 hours to see trends)\n",
    "- Available signals (need ECG for heart rate + ABP for blood pressure)\n",
    "\n",
    "**Usability criteria:**\n",
    "- ‚úÖ Duration >= 4 hours\n",
    "- ‚úÖ Has ECG signal (II or ECG)\n",
    "- ‚úÖ Has ABP signal (blood pressure)\n",
    "\n",
    "Many segments are too short or missing required signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e68fd44a-094d-47f9-8145-390eaad2f367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3000003:\n",
      "  ‚ùå 3000003_0001: 0.04h, signals: ['II', 'V']\n",
      "  ‚ùå 3000003_0002: 0.00h, signals: ['II', 'V']\n",
      "  ‚ùå 3000003_0003: 0.00h, signals: ['II', 'V']\n",
      "  ‚ùå 3000003_0004: 0.00h, signals: ['II', 'V']\n",
      "  ‚ùå 3000003_0005: 0.98h, signals: ['II', 'V']\n",
      "  ‚ùå 3000003_0006: 0.01h, signals: ['II', 'V', 'ABP']\n",
      "  ‚úÖ 3000003_0007: 4.79h, signals: ['II', 'V', 'ABP']\n",
      "  ‚ùå 3000003_0008: 0.48h, signals: ['II', 'V', 'ABP']\n",
      "  ‚ùå 3000003_0009: 1.95h, signals: ['II', 'V', 'ABP']\n",
      "  ‚ùå 3000003_0010: 2.99h, signals: ['II', 'V', 'ABP']\n",
      "  ‚ùå 3000003_0011: 0.00h, signals: ['II', 'V', 'ABP']\n",
      "  ‚ùå 3000003_0012: 0.00h, signals: ['II', 'V', 'ABP']\n",
      "  ‚ùå 3000003_0013: 1.16h, signals: ['II', 'V', 'ABP']\n",
      "  ‚úÖ 3000003_0014: 5.33h, signals: ['II', 'ABP']\n",
      "  ‚ùå 3000003_0015: 11.58h, signals: ['II']\n",
      "  ‚ùå 3000003_0016: 11.43h, signals: ['II']\n",
      "  ‚ùå 3000003_0017: 0.22h, signals: ['II', 'V']\n",
      "\n",
      "3000031:\n",
      "\n",
      "3000060:\n",
      "  ‚ùå 3000060_0001: 0.08h, signals: ['II']\n",
      "  ‚ùå 3000060_0002: 0.77h, signals: ['II', 'PLETH']\n",
      "  ‚ùå 3000060_0003: 0.00h, signals: ['PLETH']\n",
      "  ‚ùå 3000060_0004: 5.23h, signals: ['II', 'PLETH']\n",
      "  ‚ùå 3000060_0005: 0.00h, signals: ['PLETH']\n",
      "  ‚ùå 3000060_0006: 9.20h, signals: ['II', 'PLETH']\n",
      "  ‚ùå 3000060_0007: 0.00h, signals: ['PLETH']\n",
      "  ‚ùå 3000060_0008: 0.88h, signals: ['II', 'PLETH']\n",
      "  ‚ùå 3000060_0009: 0.00h, signals: ['PLETH']\n",
      "  ‚ùå 3000060_0010: 22.58h, signals: ['II', 'PLETH']\n",
      "  ‚ùå 3000060_0011: 0.00h, signals: ['PLETH']\n",
      "  ‚ùå 3000060_0012: 1.53h, signals: ['II', 'PLETH']\n",
      "  ‚ùå 3000060_0013: 0.00h, signals: ['PLETH']\n",
      "  ‚ùå 3000060_0014: 0.56h, signals: ['II', 'PLETH']\n",
      "  ‚ùå 3000060_0015: 0.00h, signals: ['PLETH']\n",
      "  ‚ùå 3000060_0016: 0.56h, signals: ['II', 'PLETH']\n",
      "  ‚ùå 3000060_0017: 0.00h, signals: ['PLETH']\n",
      "  ‚ùå 3000060_0018: 13.09h, signals: ['II', 'PLETH']\n",
      "  ‚ùå 3000060_0019: 0.00h, signals: ['PLETH']\n",
      "  ‚ùå 3000060_0020: 0.66h, signals: ['II', 'PLETH']\n",
      "\n",
      "3000063:\n",
      "  ‚ùå 3000063_0001: 0.02h, signals: ['PLETH']\n",
      "  ‚ùå 3000063_0002: 0.00h, signals: ['III']\n",
      "  ‚ùå 3000063_0003: 0.00h, signals: ['II']\n",
      "  ‚ùå 3000063_0004: 0.01h, signals: ['II']\n",
      "  ‚ùå 3000063_0005: 1.31h, signals: ['II', 'PLETH']\n",
      "  ‚ùå 3000063_0006: 1.65h, signals: ['II', 'ABP', 'PLETH']\n",
      "  ‚ùå 3000063_0007: 0.85h, signals: ['II', 'ABP', 'PLETH']\n",
      "  ‚ùå 3000063_0008: 0.00h, signals: ['II', 'ABP']\n",
      "  ‚ùå 3000063_0009: 0.08h, signals: ['II', 'ABP', 'PLETH']\n",
      "  ‚ùå 3000063_0010: 0.20h, signals: ['II', 'ABP', 'PLETH']\n",
      "  ‚ùå 3000063_0011: 0.79h, signals: ['II', 'ABP', 'PLETH']\n",
      "  ‚ùå 3000063_0012: 0.17h, signals: ['II', 'ABP', 'PLETH']\n",
      "  ‚ùå 3000063_0013: 0.27h, signals: ['II', 'ABP', 'PLETH']\n",
      "  ‚ùå 3000063_0014: 0.61h, signals: ['II', 'ABP', 'PLETH']\n",
      "  ‚ùå 3000063_0015: 0.62h, signals: ['II', 'ABP', 'PLETH']\n",
      "  ‚ùå 3000063_0016: 2.23h, signals: ['II', 'ABP', 'PLETH']\n",
      "  ‚ùå 3000063_0017: 0.01h, signals: ['II', 'ABP']\n",
      "  ‚ùå 3000063_0018: 1.34h, signals: ['II', 'ABP', 'PLETH']\n",
      "  ‚ùå 3000063_0019: 0.25h, signals: ['II', 'ABP', 'PLETH']\n",
      "  ‚úÖ 3000063_0020: 10.61h, signals: ['II', 'ABP', 'PLETH']\n",
      "  ‚ùå 3000063_0021: 0.02h, signals: ['II', 'ABP']\n",
      "  ‚úÖ 3000063_0022: 12.99h, signals: ['II', 'ABP', 'PLETH']\n",
      "  ‚ùå 3000063_0023: 3.15h, signals: ['II', 'ABP', 'PLETH']\n",
      "  ‚ùå 3000063_0024: 0.15h, signals: ['II', 'PLETH']\n",
      "  ‚úÖ 3000063_0025: 5.32h, signals: ['II', 'ABP', 'PLETH']\n",
      "  ‚ùå 3000063_0026: 0.02h, signals: ['II', 'ABP']\n",
      "  ‚ùå 3000063_0027: 0.02h, signals: ['II']\n",
      "  ‚ùå 3000063_0028: 0.03h, signals: ['II', 'PLETH']\n",
      "  ‚úÖ 3000063_0029: 19.67h, signals: ['II', 'ABP', 'PLETH']\n",
      "  ‚ùå 3000063_0030: 0.10h, signals: ['II', 'ABP']\n",
      "  ‚ùå 3000063_0031: 0.10h, signals: ['II']\n",
      "  ‚ùå 3000063_0032: 0.02h, signals: ['II', 'ABP']\n",
      "  ‚ùå 3000063_0033: 3.05h, signals: ['II', 'ABP', 'PLETH']\n",
      "  ‚ùå 3000063_0034: 0.54h, signals: ['II', 'PLETH']\n",
      "\n",
      "3000065:\n",
      "  ‚ùå 3000065_0001: 0.02h, signals: ['II']\n",
      "\n",
      "\n",
      "‚úÖ Total usable segments (>= 4hrs with ECG+BP): 6\n"
     ]
    }
   ],
   "source": [
    "# Check segment durations and signals\n",
    "usable_segments = []\n",
    "\n",
    "for patient in working_patients[:5]:  # Test with first 5\n",
    "    folder, patient_id = patient.split('/')\n",
    "    patient_path = f'data/{folder}/{patient_id}'\n",
    "    \n",
    "    files = os.listdir(patient_path)\n",
    "    segment_files = [f.replace('.hea', '') for f in files if f.endswith('.hea') and '_' in f and 'layout' not in f]\n",
    "    \n",
    "    print(f\"\\n{patient_id}:\")\n",
    "    \n",
    "    for seg_name in segment_files:\n",
    "        try:\n",
    "            # Load segment\n",
    "            record = wfdb.rdrecord(f'{patient_path}/{seg_name}')\n",
    "            duration_hrs = record.sig_len / record.fs / 3600\n",
    "            \n",
    "            # Check for ECG and BP\n",
    "            has_ecg = any('II' in s or 'ECG' in s for s in record.sig_name)\n",
    "            has_bp = any('ABP' in s or 'BP' in s for s in record.sig_name)\n",
    "            \n",
    "            status = \"‚úÖ\" if (duration_hrs >= 4 and has_ecg and has_bp) else \"‚ùå\"\n",
    "            \n",
    "            print(f\"  {status} {seg_name}: {duration_hrs:.2f}h, signals: {record.sig_name}\")\n",
    "            \n",
    "            if duration_hrs >= 4 and has_ecg and has_bp:\n",
    "                usable_segments.append(f\"{patient}/{seg_name}\")\n",
    "                \n",
    "        except:\n",
    "            print(f\"  ‚ö†Ô∏è  {seg_name}: Error loading\")\n",
    "\n",
    "print(f\"\\n\\n‚úÖ Total usable segments (>= 4hrs with ECG+BP): {len(usable_segments)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c1323b-dde3-4522-9865-30bbf8587905",
   "metadata": {},
   "source": [
    "## Step 6: Scan All 30 Working Patients\n",
    "\n",
    "Now we systematically check all 30 patients to find every usable segment that meets our criteria.\n",
    "\n",
    "**This creates our final dataset:**\n",
    "- Each usable segment becomes one training example\n",
    "- Expected: ~30-50 segments from 30 patients\n",
    "- Result: Our ML-ready dataset for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2112612-0125-4a46-bb69-914c34ea570a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Total usable segments from 30 patients: 34\n",
      "\n",
      "First 10:\n",
      "1. 30/3000003/3000003_0007 - 4.8h\n",
      "2. 30/3000003/3000003_0014 - 5.3h\n",
      "3. 30/3000063/3000063_0020 - 10.6h\n",
      "4. 30/3000063/3000063_0022 - 13.0h\n",
      "5. 30/3000063/3000063_0025 - 5.3h\n",
      "6. 30/3000063/3000063_0029 - 19.7h\n",
      "7. 30/3000126/3000126_0007 - 14.1h\n",
      "8. 30/3000126/3000126_0009 - 7.0h\n",
      "9. 30/3000126/3000126_0012 - 13.9h\n",
      "10. 30/3000126/3000126_0013 - 18.1h\n"
     ]
    }
   ],
   "source": [
    "# Check all 30 working patients\n",
    "all_usable_segments = []\n",
    "\n",
    "for patient in working_patients:\n",
    "    folder, patient_id = patient.split('/')\n",
    "    patient_path = f'data/{folder}/{patient_id}'\n",
    "    \n",
    "    try:\n",
    "        files = os.listdir(patient_path)\n",
    "        segment_files = [f.replace('.hea', '') for f in files \n",
    "                        if f.endswith('.hea') and '_' in f and 'layout' not in f]\n",
    "        \n",
    "        for seg_name in segment_files:\n",
    "            try:\n",
    "                record = wfdb.rdrecord(f'{patient_path}/{seg_name}')\n",
    "                duration_hrs = record.sig_len / record.fs / 3600\n",
    "                \n",
    "                has_ecg = any('II' in s or 'ECG' in s for s in record.sig_name)\n",
    "                has_bp = any('ABP' in s or 'BP' in s for s in record.sig_name)\n",
    "                \n",
    "                if duration_hrs >= 4 and has_ecg and has_bp:\n",
    "                    all_usable_segments.append({\n",
    "                        'patient': patient,\n",
    "                        'segment': seg_name,\n",
    "                        'duration': duration_hrs,\n",
    "                        'signals': record.sig_name\n",
    "                    })\n",
    "            except:\n",
    "                pass\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(f\"‚úÖ Total usable segments from 30 patients: {len(all_usable_segments)}\")\n",
    "print(f\"\\nFirst 10:\")\n",
    "for i, seg in enumerate(all_usable_segments[:10]):\n",
    "    print(f\"{i+1}. {seg['patient']}/{seg['segment']} - {seg['duration']:.1f}h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b451dfa8-0f92-4c1f-8a62-edf2becd0e79",
   "metadata": {},
   "source": [
    "## Step 7: Save Usable Segments List\n",
    "\n",
    "Save the list of usable segments to a CSV file so we can easily load it in future sessions without re-scanning all patients.\n",
    "\n",
    "This gives us a permanent record of our 34 usable segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0a60170-b262-4845-bc88-47be30bdef6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved to: usable_segments.csv\n",
      "\n",
      "You have 34 segments to work with!\n"
     ]
    }
   ],
   "source": [
    "# Save the usable segments list\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(all_usable_segments)\n",
    "df.to_csv('usable_segments.csv', index=False)\n",
    "\n",
    "print(\"‚úÖ Saved to: usable_segments.csv\")\n",
    "print(f\"\\nYou have {len(df)} segments to work with!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede7d65a-7bba-4627-8c73-adce88b73f65",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "\n",
    "Now we'll extract meaningful features from raw waveform data. Each segment contains millions of data points - we need to compress this into ~20-30 features that ML models can use.\n",
    "\n",
    "## Step 8: Load Example Segment\n",
    "\n",
    "Load the first usable segment to develop and test our feature extraction pipeline.\n",
    "\n",
    "**What we'll see:**\n",
    "- Patient ID and segment name\n",
    "- Recording duration\n",
    "- Available signals (ECG, BP, etc.)\n",
    "- Sampling rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b00b0223-4c08-42c4-b833-9e396933f73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: 30/3000003/3000003_0007\n",
      "Duration: 4.79 hours\n",
      "Signals: ['II', 'V', 'ABP']\n",
      "Sampling rate: 125 Hz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import wfdb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the first usable segment\n",
    "seg = all_usable_segments[0]\n",
    "patient_path = f\"data/{seg['patient'].split('/')[0]}/{seg['patient'].split('/')[1]}\"\n",
    "seg_name = seg['segment']\n",
    "\n",
    "print(f\"Loading: {seg['patient']}/{seg_name}\")\n",
    "print(f\"Duration: {seg['duration']:.2f} hours\")\n",
    "\n",
    "# Load the record\n",
    "record = wfdb.rdrecord(f\"{patient_path}/{seg_name}\")\n",
    "print(f\"Signals: {record.sig_name}\")\n",
    "print(f\"Sampling rate: {record.fs} Hz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac5eaaf-681b-43e0-bfa6-279a76f187d6",
   "metadata": {},
   "source": [
    "## Step 9: Extract Heart Rate from ECG\n",
    "\n",
    "**Process:**\n",
    "1. Load ECG signal (Lead II)\n",
    "2. Detect R-peaks (the tall spikes in ECG = heartbeats)\n",
    "3. Calculate time between peaks (RR intervals)\n",
    "4. Convert to beats per minute (BPM)\n",
    "\n",
    "**Features extracted:**\n",
    "- Mean HR, Min HR, Max HR, Standard deviation\n",
    "\n",
    "These statistics tell us if the heart rate is normal, abnormal, or highly variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36a97f83-c76d-449a-b759-37d6f5555457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total heartbeats detected: 19887\n",
      "Heart rate statistics:\n",
      "  Mean: 69.5 bpm\n",
      "  Min: 31.2 bpm\n",
      "  Max: 119.0 bpm\n",
      "  Std: 4.3 bpm\n"
     ]
    }
   ],
   "source": [
    "# Get ECG signal (Lead II)\n",
    "ecg = record.p_signal[:, 0]  # First signal = Lead II\n",
    "\n",
    "# Simple heart rate extraction\n",
    "# Count peaks in ECG to get heart rate\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# Find R-peaks in ECG (the tall spikes)\n",
    "peaks, _ = find_peaks(ecg, distance=record.fs*0.5, height=0.3)\n",
    "\n",
    "# Calculate heart rate from peaks\n",
    "# Time between peaks = RR interval\n",
    "rr_intervals = np.diff(peaks) / record.fs  # in seconds\n",
    "heart_rates = 60 / rr_intervals  # convert to beats per minute\n",
    "\n",
    "print(f\"Total heartbeats detected: {len(peaks)}\")\n",
    "print(f\"Heart rate statistics:\")\n",
    "print(f\"  Mean: {np.mean(heart_rates):.1f} bpm\")\n",
    "print(f\"  Min: {np.min(heart_rates):.1f} bpm\")\n",
    "print(f\"  Max: {np.max(heart_rates):.1f} bpm\")\n",
    "print(f\"  Std: {np.std(heart_rates):.1f} bpm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb2e62f-f67e-4f74-8b00-fcd509efe9e7",
   "metadata": {},
   "source": [
    "## Step 10: Extract Blood Pressure from ABP\n",
    "\n",
    "**Process:**\n",
    "1. Load ABP (Arterial Blood Pressure) signal\n",
    "2. Detect systolic peaks (high points in waveform)\n",
    "3. Detect diastolic troughs (low points in waveform)\n",
    "4. Calculate statistics\n",
    "\n",
    "**Clinical significance:**\n",
    "- **Systolic:** Peak pressure when heart contracts\n",
    "- **Diastolic:** Minimum pressure when heart relaxes\n",
    "\n",
    "Normal ranges: Systolic ~120 mmHg, Diastolic ~80 mmHg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a027387c-b41f-466e-9a54-39d8f3b1a527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blood Pressure statistics:\n",
      "  Systolic mean: 102.0 mmHg\n",
      "  Systolic min: 50.4 mmHg\n",
      "  Systolic max: 180.0 mmHg\n",
      "  Diastolic mean: 58.5 mmHg\n"
     ]
    }
   ],
   "source": [
    "# Get ABP signal (Arterial Blood Pressure)\n",
    "abp = record.p_signal[:, 2]  # Third signal = ABP\n",
    "\n",
    "# Find systolic peaks (high points)\n",
    "systolic_peaks, _ = find_peaks(abp, distance=record.fs*0.5, height=50)\n",
    "systolic_values = abp[systolic_peaks]\n",
    "\n",
    "# Find diastolic troughs (low points)\n",
    "# Invert signal to find valleys\n",
    "diastolic_peaks, _ = find_peaks(-abp, distance=record.fs*0.5)\n",
    "diastolic_values = abp[diastolic_peaks]\n",
    "\n",
    "print(f\"Blood Pressure statistics:\")\n",
    "print(f\"  Systolic mean: {np.mean(systolic_values):.1f} mmHg\")\n",
    "print(f\"  Systolic min: {np.min(systolic_values):.1f} mmHg\")\n",
    "print(f\"  Systolic max: {np.max(systolic_values):.1f} mmHg\")\n",
    "print(f\"  Diastolic mean: {np.mean(diastolic_values):.1f} mmHg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a284a31c-fd9a-4eda-8de2-5fb05fb41afe",
   "metadata": {},
   "source": [
    "## Step 11: Calculate Trends (Early vs Late)\n",
    "\n",
    "**Critical for deterioration detection!**\n",
    "\n",
    "Compare the first half vs second half of the recording to detect changes over time:\n",
    "- Is heart rate increasing or decreasing?\n",
    "- Is blood pressure rising or falling?\n",
    "- What is the magnitude of change?\n",
    "\n",
    "**Clinical patterns:**\n",
    "- ‚úÖ **Stable:** Minimal changes (<5 bpm HR, <10 mmHg BP)\n",
    "- ‚ö†Ô∏è **HR increasing + BP decreasing:** Possible compensatory shock/deterioration\n",
    "- üìä **Other changes:** May indicate clinical events requiring investigation\n",
    "\n",
    "**Why this matters:**\n",
    "Deterioration happens gradually over hours. Comparing early vs late reveals trends that single-time measurements miss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8975e6b4-ebe5-4791-b13c-ba23888aa01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRENDS (First Half ‚Üí Second Half):\n",
      "\n",
      "Heart Rate:\n",
      "  Early: 69.5 bpm\n",
      "  Late: 69.4 bpm\n",
      "  Change: -0.1 bpm (-0.1%)\n",
      "\n",
      "Blood Pressure:\n",
      "  Early: 101.8 mmHg\n",
      "  Late: 102.3 mmHg\n",
      "  Change: +0.4 mmHg (+0.4%)\n",
      "\n",
      "‚úÖ PATTERN: Stable (minimal changes)\n"
     ]
    }
   ],
   "source": [
    "# Split data into first half vs second half\n",
    "midpoint_time = len(heart_rates) // 2\n",
    "midpoint_bp = len(systolic_values) // 2\n",
    "\n",
    "# Heart rate: early vs late\n",
    "hr_early = heart_rates[:midpoint_time]\n",
    "hr_late = heart_rates[midpoint_time:]\n",
    "\n",
    "hr_early_mean = np.mean(hr_early)\n",
    "hr_late_mean = np.mean(hr_late)\n",
    "hr_change = hr_late_mean - hr_early_mean\n",
    "hr_percent_change = (hr_change / hr_early_mean) * 100\n",
    "\n",
    "# Blood pressure: early vs late\n",
    "bp_early = systolic_values[:midpoint_bp]\n",
    "bp_late = systolic_values[midpoint_bp:]\n",
    "\n",
    "bp_early_mean = np.mean(bp_early)\n",
    "bp_late_mean = np.mean(bp_late)\n",
    "bp_change = bp_late_mean - bp_early_mean\n",
    "bp_percent_change = (bp_change / bp_early_mean) * 100\n",
    "\n",
    "print(\"TRENDS (First Half ‚Üí Second Half):\")\n",
    "print(f\"\\nHeart Rate:\")\n",
    "print(f\"  Early: {hr_early_mean:.1f} bpm\")\n",
    "print(f\"  Late: {hr_late_mean:.1f} bpm\")\n",
    "print(f\"  Change: {hr_change:+.1f} bpm ({hr_percent_change:+.1f}%)\")\n",
    "\n",
    "print(f\"\\nBlood Pressure:\")\n",
    "print(f\"  Early: {bp_early_mean:.1f} mmHg\")\n",
    "print(f\"  Late: {bp_late_mean:.1f} mmHg\")\n",
    "print(f\"  Change: {bp_change:+.1f} mmHg ({bp_percent_change:+.1f}%)\")\n",
    "\n",
    "# Clinical interpretation\n",
    "if hr_change > 0 and bp_change < 0:\n",
    "    print(\"\\n‚ö†Ô∏è PATTERN: HR increasing + BP decreasing (possible deterioration)\")\n",
    "elif abs(hr_change) < 5 and abs(bp_change) < 10:\n",
    "    print(\"\\n‚úÖ PATTERN: Stable (minimal changes)\")\n",
    "else:\n",
    "    print(\"\\nüìä PATTERN: Changes detected, further analysis needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca17cdd8-5ef1-4ad7-846b-27b5849a8cc5",
   "metadata": {},
   "source": [
    "## Step 12: Create Reusable Feature Extraction Function\n",
    "\n",
    "Package all extraction logic into a single function that can process any segment.\n",
    "\n",
    "**What this function does:**\n",
    "1. Loads a segment's waveform data\n",
    "2. Extracts heart rate from ECG signal\n",
    "3. Extracts blood pressure from ABP signal\n",
    "4. Calculates trends (early vs late comparison)\n",
    "5. Computes clinical metrics (shock index, etc.)\n",
    "6. Returns a dictionary of ~15-20 features\n",
    "\n",
    "**Why we need this:**\n",
    "We have 34 segments to process. This function lets us extract features from all of them systematically and consistently.\n",
    "\n",
    "**Output:** One feature dictionary per segment (ready for ML training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d10ffd5-480e-4cf0-866b-02e9ad213256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(patient, segment_name):\n",
    "    \"\"\"Extract features from one segment - with better error handling\"\"\"\n",
    "    \n",
    "    # Load record\n",
    "    folder, patient_id = patient.split('/')\n",
    "    record = wfdb.rdrecord(f\"data/{folder}/{patient_id}/{segment_name}\")\n",
    "    \n",
    "    # Find ECG signal (could be at different positions)\n",
    "    ecg_idx = None\n",
    "    for i, name in enumerate(record.sig_name):\n",
    "        if 'II' in name or 'ECG' in name:\n",
    "            ecg_idx = i\n",
    "            break\n",
    "    \n",
    "    # Find ABP signal (could be at different positions)\n",
    "    abp_idx = None\n",
    "    for i, name in enumerate(record.sig_name):\n",
    "        if 'ABP' in name or 'BP' in name:\n",
    "            abp_idx = i\n",
    "            break\n",
    "    \n",
    "    if ecg_idx is None or abp_idx is None:\n",
    "        raise ValueError(f\"Missing required signals. Available: {record.sig_name}\")\n",
    "    \n",
    "    # Get signals\n",
    "    ecg = record.p_signal[:, ecg_idx]\n",
    "    abp = record.p_signal[:, abp_idx]\n",
    "    \n",
    "    # Extract heart rate\n",
    "    peaks, _ = find_peaks(ecg, distance=record.fs*0.5, height=0.3)\n",
    "    \n",
    "    if len(peaks) < 10:  # Need at least 10 heartbeats\n",
    "        raise ValueError(f\"Too few heartbeats detected: {len(peaks)}\")\n",
    "    \n",
    "    rr_intervals = np.diff(peaks) / record.fs\n",
    "    heart_rates = 60 / rr_intervals\n",
    "    \n",
    "    # Extract blood pressure\n",
    "    systolic_peaks, _ = find_peaks(abp, distance=record.fs*0.5, height=50)\n",
    "    \n",
    "    if len(systolic_peaks) < 10:  # Need at least 10 BP measurements\n",
    "        raise ValueError(f\"Too few BP peaks detected: {len(systolic_peaks)}\")\n",
    "    \n",
    "    systolic_values = abp[systolic_peaks]\n",
    "    \n",
    "    # Calculate trends (early vs late)\n",
    "    midpoint_hr = len(heart_rates) // 2\n",
    "    midpoint_bp = len(systolic_values) // 2\n",
    "    \n",
    "    hr_early_mean = np.mean(heart_rates[:midpoint_hr])\n",
    "    hr_late_mean = np.mean(heart_rates[midpoint_hr:])\n",
    "    \n",
    "    bp_early_mean = np.mean(systolic_values[:midpoint_bp])\n",
    "    bp_late_mean = np.mean(systolic_values[midpoint_bp:])\n",
    "    \n",
    "    # Create feature dictionary\n",
    "    features = {\n",
    "        'patient_id': patient,\n",
    "        'segment': segment_name,\n",
    "        'duration_hours': record.sig_len / record.fs / 3600,\n",
    "        \n",
    "        # Heart rate features\n",
    "        'hr_mean': np.mean(heart_rates),\n",
    "        'hr_std': np.std(heart_rates),\n",
    "        'hr_min': np.min(heart_rates),\n",
    "        'hr_max': np.max(heart_rates),\n",
    "        'hr_early_mean': hr_early_mean,\n",
    "        'hr_late_mean': hr_late_mean,\n",
    "        'hr_change': hr_late_mean - hr_early_mean,\n",
    "        'hr_percent_change': ((hr_late_mean - hr_early_mean) / hr_early_mean) * 100,\n",
    "        \n",
    "        # Blood pressure features\n",
    "        'bp_systolic_mean': np.mean(systolic_values),\n",
    "        'bp_systolic_std': np.std(systolic_values),\n",
    "        'bp_systolic_min': np.min(systolic_values),\n",
    "        'bp_systolic_max': np.max(systolic_values),\n",
    "        'bp_early_mean': bp_early_mean,\n",
    "        'bp_late_mean': bp_late_mean,\n",
    "        'bp_change': bp_late_mean - bp_early_mean,\n",
    "        'bp_percent_change': ((bp_late_mean - bp_early_mean) / bp_early_mean) * 100,\n",
    "        \n",
    "        # Clinical metrics\n",
    "        'shock_index': np.mean(heart_rates) / np.mean(systolic_values)\n",
    "    }\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decbad41-b5c6-4ade-ab96-1fa26aa4011a",
   "metadata": {},
   "source": [
    "## Step 13: Extract Features from All Segments\n",
    "\n",
    "Apply the feature extraction function to all 34 usable segments. This creates our ML-ready dataset.\n",
    "\n",
    "**What happens:**\n",
    "- Process each of the 34 segments\n",
    "- Extract 20 features per segment\n",
    "- Handle any errors gracefully (some segments might fail)\n",
    "- Save results to CSV\n",
    "\n",
    "**Result:** A dataset with 34 rows (segments) √ó 20 columns (features)\n",
    "\n",
    "**Processing time:** ~2-5 minutes for 34 segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5730eb7-13e2-4622-b637-0e9c2f973666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from all 34 segments (with improved error handling)...\n",
      "‚úÖ Processed 5/34\n",
      "‚úÖ Processed 10/34\n",
      "‚úÖ Processed 15/34\n",
      "‚úÖ Processed 20/34\n",
      "‚úÖ Processed 25/34\n",
      "‚úÖ Processed 30/34\n",
      "\n",
      "‚úÖ Complete! Successfully processed: 34/34\n",
      "üíæ Saved 34 segments to: extracted_features.csv\n"
     ]
    }
   ],
   "source": [
    "# Extract features from all segments (WITH FIXED FUNCTION)\n",
    "print(\"Extracting features from all 34 segments (with improved error handling)...\")\n",
    "\n",
    "all_features = []\n",
    "\n",
    "for i, seg_info in enumerate(all_usable_segments):\n",
    "    try:\n",
    "        features = extract_features(seg_info['patient'], seg_info['segment'])\n",
    "        all_features.append(features)\n",
    "        if (i + 1) % 5 == 0:\n",
    "            print(f\"‚úÖ Processed {i + 1}/{len(all_usable_segments)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {seg_info['patient']}/{seg_info['segment']}: {str(e)}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Complete! Successfully processed: {len(all_features)}/{len(all_usable_segments)}\")\n",
    "\n",
    "features_df = pd.DataFrame(all_features)\n",
    "features_df.to_csv('extracted_features.csv', index=False)\n",
    "print(f\"üíæ Saved {len(features_df)} segments to: extracted_features.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd487681-4c46-44ba-8615-e043107bc69b",
   "metadata": {},
   "source": [
    "# Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5465acb2-3054-410c-a9b5-5a100929f9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Dataset Summary:\n",
      "   Segments: 34\n",
      "   Features: 20\n",
      "\n",
      "üìã Feature names:\n",
      "   - patient_id\n",
      "   - segment\n",
      "   - duration_hours\n",
      "   - hr_mean\n",
      "   - hr_std\n",
      "   - hr_min\n",
      "   - hr_max\n",
      "   - hr_early_mean\n",
      "   - hr_late_mean\n",
      "   - hr_change\n",
      "   - hr_percent_change\n",
      "   - bp_systolic_mean\n",
      "   - bp_systolic_std\n",
      "   - bp_systolic_min\n",
      "   - bp_systolic_max\n",
      "   - bp_early_mean\n",
      "   - bp_late_mean\n",
      "   - bp_change\n",
      "   - bp_percent_change\n",
      "   - shock_index\n",
      "\n",
      "üìà Statistical Summary:\n",
      "          hr_mean  hr_change  bp_systolic_mean  bp_change  shock_index\n",
      "count   34.000000  34.000000         34.000000  34.000000    34.000000\n",
      "mean    80.226847   1.065668        124.807971   1.869751     0.650514\n",
      "std     11.046956   8.882720         14.527857   8.878541     0.112361\n",
      "min     62.421025 -19.645487        100.509744 -16.690726     0.468115\n",
      "25%     72.691654  -2.515789        112.515052  -3.953715     0.579054\n",
      "50%     78.623016  -0.136152        125.156093   1.947831     0.636076\n",
      "75%     85.990876   5.831549        134.957482   7.704181     0.718521\n",
      "max    112.330839  35.405384        153.341455  20.777290     0.948521\n"
     ]
    }
   ],
   "source": [
    "# Load and examine the dataset\n",
    "features_df = pd.read_csv('extracted_features.csv')\n",
    "\n",
    "print(f\"üìä Dataset Summary:\")\n",
    "print(f\"   Segments: {len(features_df)}\")\n",
    "print(f\"   Features: {len(features_df.columns)}\")\n",
    "print(f\"\\nüìã Feature names:\")\n",
    "for col in features_df.columns:\n",
    "    print(f\"   - {col}\")\n",
    "\n",
    "print(f\"\\nüìà Statistical Summary:\")\n",
    "print(features_df[['hr_mean', 'hr_change', 'bp_systolic_mean', 'bp_change', 'shock_index']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da25bfa-512e-4dc8-aa63-2034601e28da",
   "metadata": {},
   "source": [
    "# Data Labeling\n",
    "\n",
    "Now we need to label each segment: Did the patient deteriorate or stay stable?\n",
    "\n",
    "## Step 14: Rule-Based Labeling\n",
    "\n",
    "We'll use clinical patterns to create labels:\n",
    "\n",
    "**Deterioration indicators:**\n",
    "- ‚ö†Ô∏è HR increasing + BP decreasing (compensatory shock)\n",
    "- ‚ö†Ô∏è HR very high (>100 bpm) and rising\n",
    "- ‚ö†Ô∏è BP very low (<100 mmHg) and falling\n",
    "- ‚ö†Ô∏è High shock index (>0.9)\n",
    "\n",
    "**Stable indicators:**\n",
    "- ‚úÖ Minimal changes in HR and BP\n",
    "- ‚úÖ Normal ranges maintained\n",
    "- ‚úÖ Low shock index (<0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d6d1247-b093-42c8-bc5b-a80ed947fcb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Label Distribution:\n",
      "   Stable (0): 30 segments\n",
      "   Deteriorating (1): 4 segments\n",
      "\n",
      "   Deterioration rate: 11.8%\n",
      "\n",
      "üíæ Saved labeled dataset to: labeled_features.csv\n",
      "\n",
      "‚úÖ Example STABLE segments:\n",
      "   patient_id     hr_mean  hr_change  bp_systolic_mean  bp_change  shock_index\n",
      "0  30/3000003   69.467815  -0.062783        102.030018   0.448434     0.680857\n",
      "1  30/3000003   66.065046   5.424126        100.509744  13.713921     0.657300\n",
      "5  30/3000063  103.762332 -11.996534        128.044793   9.685414     0.810360\n",
      "\n",
      "‚ö†Ô∏è  Example DETERIORATING segments:\n",
      "   patient_id     hr_mean  hr_change  bp_systolic_mean  bp_change  shock_index\n",
      "2  30/3000063   98.907667   6.456491        109.826543 -14.026278     0.900581\n",
      "3  30/3000063   79.143347  35.405384        117.546987  11.274592     0.673291\n",
      "4  30/3000063  112.330839  -2.551825        118.427341  -4.305803     0.948521\n"
     ]
    }
   ],
   "source": [
    "# Create labels based on clinical patterns\n",
    "def label_segment(row):\n",
    "    \"\"\"\n",
    "    Label segment as deteriorating (1) or stable (0)\n",
    "    Based on clinical deterioration patterns\n",
    "    \"\"\"\n",
    "    \n",
    "    # Pattern 1: HR increasing AND BP decreasing (classic deterioration)\n",
    "    if row['hr_change'] > 10 and row['bp_change'] < -5:\n",
    "        return 1\n",
    "    \n",
    "    # Pattern 2: Very high HR (tachycardia) and getting worse\n",
    "    if row['hr_mean'] > 100 and row['hr_change'] > 5:\n",
    "        return 1\n",
    "    \n",
    "    # Pattern 3: Low BP (hypotension) and getting worse\n",
    "    if row['bp_systolic_mean'] < 100 and row['bp_change'] < -5:\n",
    "        return 1\n",
    "    \n",
    "    # Pattern 4: High shock index (>0.9 = concerning)\n",
    "    if row['shock_index'] > 0.9:\n",
    "        return 1\n",
    "    \n",
    "    # Pattern 5: Large HR increase (>20 bpm)\n",
    "    if row['hr_change'] > 20:\n",
    "        return 1\n",
    "    \n",
    "    # Pattern 6: Large BP drop (>15 mmHg)\n",
    "    if row['bp_change'] < -15:\n",
    "        return 1\n",
    "    \n",
    "    # Otherwise: Stable\n",
    "    return 0\n",
    "\n",
    "# Apply labeling\n",
    "features_df['deterioration'] = features_df.apply(label_segment, axis=1)\n",
    "\n",
    "# Check distribution\n",
    "print(\"üìä Label Distribution:\")\n",
    "print(f\"   Stable (0): {(features_df['deterioration'] == 0).sum()} segments\")\n",
    "print(f\"   Deteriorating (1): {(features_df['deterioration'] == 1).sum()} segments\")\n",
    "print(f\"\\n   Deterioration rate: {features_df['deterioration'].mean()*100:.1f}%\")\n",
    "\n",
    "# Save labeled dataset\n",
    "features_df.to_csv('labeled_features.csv', index=False)\n",
    "print(f\"\\nüíæ Saved labeled dataset to: labeled_features.csv\")\n",
    "\n",
    "# Show examples of each class\n",
    "print(\"\\n‚úÖ Example STABLE segments:\")\n",
    "print(features_df[features_df['deterioration']==0][['patient_id', 'hr_mean', 'hr_change', 'bp_systolic_mean', 'bp_change', 'shock_index']].head(3))\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  Example DETERIORATING segments:\")\n",
    "print(features_df[features_df['deterioration']==1][['patient_id', 'hr_mean', 'hr_change', 'bp_systolic_mean', 'bp_change', 'shock_index']].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a63986b-954f-42ec-964d-945b51dd48df",
   "metadata": {},
   "source": [
    "# Finding More Deteriorating Patients\n",
    "\n",
    "With only 4 deteriorating segments, our ML model can't learn effectively. Let's scan more patients to find more deterioration cases.\n",
    "\n",
    "## Step 16: Scan Additional Patients for Deterioration\n",
    "\n",
    "We'll process 50 more patients (total 80) and keep segments that show deterioration patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5099184-90d4-42fd-be17-beb112ec48ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Scanning 50 additional patients for deterioration...\n",
      "Looking specifically for segments with deterioration patterns...\n",
      "This may take 5-10 minutes...\n",
      "\n",
      "‚úÖ Found deteriorating: 30/3000717/3000717_0059\n",
      "‚úÖ Found deteriorating: 30/3000717/3000717_0069\n",
      "‚úÖ Found deteriorating: 30/3000717/3000717_0077\n",
      "‚úÖ Found deteriorating: 30/3000717/3000717_0078\n",
      "Scanned 10/50 patients... Found 4 deteriorating\n",
      "‚úÖ Found deteriorating: 30/3000860/3000860_0007\n",
      "Scanned 20/50 patients... Found 5 deteriorating\n",
      "‚úÖ Found deteriorating: 30/3001099/3001099_0008\n",
      "Scanned 30/50 patients... Found 6 deteriorating\n",
      "‚úÖ Found deteriorating: 30/3001203/3001203_0018\n",
      "‚úÖ Found deteriorating: 30/3001203/3001203_0044\n",
      "Scanned 40/50 patients... Found 8 deteriorating\n",
      "‚úÖ Found deteriorating: 30/3001281/3001281_0003\n",
      "Scanned 50/50 patients... Found 9 deteriorating\n",
      "\n",
      "‚úÖ Scan complete!\n",
      "   New deteriorating segments found: 9\n",
      "   Total new segments: 44\n"
     ]
    }
   ],
   "source": [
    "# Use next 50 patients from our downloaded list\n",
    "additional_patients = downloaded[30:80]  # Patients 31-80\n",
    "\n",
    "print(f\"üîç Scanning {len(additional_patients)} additional patients for deterioration...\")\n",
    "print(\"Looking specifically for segments with deterioration patterns...\")\n",
    "print(\"This may take 5-10 minutes...\\n\")\n",
    "\n",
    "new_segments = []\n",
    "deteriorating_found = 0\n",
    "\n",
    "for i, patient in enumerate(additional_patients):\n",
    "    folder, patient_id = patient.split('/')\n",
    "    patient_path = f'data/{folder}/{patient_id}'\n",
    "    \n",
    "    try:\n",
    "        files = os.listdir(patient_path)\n",
    "        segment_files = [f.replace('.hea', '') for f in files \n",
    "                        if f.endswith('.hea') and '_' in f and 'layout' not in f]\n",
    "        \n",
    "        for seg_name in segment_files:\n",
    "            try:\n",
    "                # Quick check: load and verify\n",
    "                record = wfdb.rdrecord(f'{patient_path}/{seg_name}')\n",
    "                duration_hrs = record.sig_len / record.fs / 3600\n",
    "                \n",
    "                # Only process segments >= 4 hours with ECG + BP\n",
    "                has_ecg = any('II' in s or 'ECG' in s for s in record.sig_name)\n",
    "                has_bp = any('ABP' in s or 'BP' in s for s in record.sig_name)\n",
    "                \n",
    "                if duration_hrs >= 4 and has_ecg and has_bp:\n",
    "                    # Extract features\n",
    "                    features = extract_features(patient, seg_name)\n",
    "                    \n",
    "                    # Check if shows deterioration\n",
    "                    label = label_segment(features)\n",
    "                    features['deterioration'] = label\n",
    "                    \n",
    "                    if label == 1:  # Deteriorating!\n",
    "                        new_segments.append(features)\n",
    "                        deteriorating_found += 1\n",
    "                        print(f\"‚úÖ Found deteriorating: {patient}/{seg_name}\")\n",
    "                    elif len(new_segments) < 50:  # Also keep some stable for balance\n",
    "                        new_segments.append(features)\n",
    "                        \n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Scanned {i + 1}/{len(additional_patients)} patients... Found {deteriorating_found} deteriorating\")\n",
    "            \n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(f\"\\n‚úÖ Scan complete!\")\n",
    "print(f\"   New deteriorating segments found: {deteriorating_found}\")\n",
    "print(f\"   Total new segments: {len(new_segments)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a54c6d9f-a832-49e1-be8f-71f8375a20aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Scanning patients 80-200 for DETERIORATING segments only...\n",
      "Target: Find ~20-30 more deteriorating cases\n",
      "\n",
      "‚úÖ Found #1: 30/3001570/3001570_0002\n",
      "Scanned 20/120 patients... Found 1 deteriorating\n",
      "‚úÖ Found #2: 30/3001937/3001937_0002\n",
      "‚úÖ Found #3: 30/3001937/3001937_0009\n",
      "‚úÖ Found #4: 30/3002090/3002090_0008\n",
      "Scanned 40/120 patients... Found 4 deteriorating\n",
      "Scanned 60/120 patients... Found 4 deteriorating\n",
      "‚úÖ Found #5: 31/3100038/3100038_0117\n",
      "‚úÖ Found #6: 31/3100038/3100038_0157\n",
      "Scanned 80/120 patients... Found 6 deteriorating\n",
      "‚úÖ Found #7: 31/3100198/3100198_0013\n",
      "‚úÖ Found #8: 31/3100198/3100198_0015\n",
      "‚úÖ Found #9: 31/3100237/3100237_0043\n",
      "‚úÖ Found #10: 31/3100240/3100240_0006\n",
      "‚úÖ Found #11: 31/3100305/3100305_0004\n",
      "Scanned 100/120 patients... Found 11 deteriorating\n",
      "‚úÖ Found #12: 31/3100461/3100461_0010\n",
      "‚úÖ Found #13: 31/3100618/3100618_0030\n",
      "Scanned 120/120 patients... Found 13 deteriorating\n",
      "\n",
      "‚úÖ Scan complete!\n",
      "   Total deteriorating segments found: 13\n"
     ]
    }
   ],
   "source": [
    "# Scan MORE patients, ONLY keep deteriorating segments\n",
    "print(f\"üîç Scanning patients 80-200 for DETERIORATING segments only...\")\n",
    "print(\"Target: Find ~20-30 more deteriorating cases\\n\")\n",
    "\n",
    "more_patients = downloaded[80:200]  # Next 120 patients\n",
    "deteriorating_segments = []\n",
    "\n",
    "for i, patient in enumerate(more_patients):\n",
    "    folder, patient_id = patient.split('/')\n",
    "    patient_path = f'data/{folder}/{patient_id}'\n",
    "    \n",
    "    try:\n",
    "        files = os.listdir(patient_path)\n",
    "        segment_files = [f.replace('.hea', '') for f in files \n",
    "                        if f.endswith('.hea') and '_' in f and 'layout' not in f]\n",
    "        \n",
    "        for seg_name in segment_files:\n",
    "            try:\n",
    "                record = wfdb.rdrecord(f'{patient_path}/{seg_name}')\n",
    "                duration_hrs = record.sig_len / record.fs / 3600\n",
    "                \n",
    "                has_ecg = any('II' in s or 'ECG' in s for s in record.sig_name)\n",
    "                has_bp = any('ABP' in s or 'BP' in s for s in record.sig_name)\n",
    "                \n",
    "                if duration_hrs >= 4 and has_ecg and has_bp:\n",
    "                    features = extract_features(patient, seg_name)\n",
    "                    label = label_segment(features)\n",
    "                    \n",
    "                    # ONLY keep deteriorating!\n",
    "                    if label == 1:\n",
    "                        features['deterioration'] = label\n",
    "                        deteriorating_segments.append(features)\n",
    "                        print(f\"‚úÖ Found #{len(deteriorating_segments)}: {patient}/{seg_name}\")\n",
    "                        \n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "        if (i + 1) % 20 == 0:\n",
    "            print(f\"Scanned {i + 1}/{len(more_patients)} patients... Found {len(deteriorating_segments)} deteriorating\")\n",
    "            \n",
    "        # Stop if we found enough\n",
    "        if len(deteriorating_segments) >= 30:\n",
    "            print(f\"\\nüéØ Target reached! Found {len(deteriorating_segments)} deteriorating segments\")\n",
    "            break\n",
    "            \n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(f\"\\n‚úÖ Scan complete!\")\n",
    "print(f\"   Total deteriorating segments found: {len(deteriorating_segments)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "975e7e6d-9be6-4e1c-b92b-467165e0bcef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Available CSV files:\n",
      "  - complete_features_all_vitals.csv\n",
      "  - complete_segments_with_all_vitals.csv\n",
      "  - extracted_features.csv\n",
      "  - FINAL_COMPLETE_DATASET.csv\n",
      "  - labeled_features.csv\n",
      "  - MASTER_DATASET.csv\n",
      "  - ML_CLAUDE_EXPLANATIONS.csv\n",
      "  - usable_segments.csv\n",
      "\n",
      "‚úÖ Loaded labeled_features.csv: 34 segments\n",
      "‚úÖ new_segments in memory: 44\n",
      "Combined: 78 segments\n",
      "‚úÖ deteriorating_segments in memory: 13\n",
      "Combined: 91 segments\n",
      "\n",
      "üíæ Created MASTER_DATASET.csv: 91 segments\n",
      "   Stable: 65\n",
      "   Deteriorating: 26\n"
     ]
    }
   ],
   "source": [
    "# First, check what we have\n",
    "print(\"üìÅ Available CSV files:\")\n",
    "import os\n",
    "for f in os.listdir('.'):\n",
    "    if f.endswith('.csv'):\n",
    "        print(f\"  - {f}\")\n",
    "\n",
    "# Load what exists\n",
    "labeled = pd.read_csv('labeled_features.csv')\n",
    "print(f\"\\n‚úÖ Loaded labeled_features.csv: {len(labeled)} segments\")\n",
    "\n",
    "# Check if we have the additional data in memory\n",
    "if 'new_segments' in locals() and len(new_segments) > 0:\n",
    "    print(f\"‚úÖ new_segments in memory: {len(new_segments)}\")\n",
    "    all_data = pd.concat([labeled, pd.DataFrame(new_segments)], ignore_index=True)\n",
    "    print(f\"Combined: {len(all_data)} segments\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Only using labeled_features.csv (no new_segments in memory)\")\n",
    "    all_data = labeled\n",
    "\n",
    "if 'deteriorating_segments' in locals() and len(deteriorating_segments) > 0:\n",
    "    print(f\"‚úÖ deteriorating_segments in memory: {len(deteriorating_segments)}\")\n",
    "    all_data = pd.concat([all_data, pd.DataFrame(deteriorating_segments)], ignore_index=True)\n",
    "    print(f\"Combined: {len(all_data)} segments\")\n",
    "\n",
    "# Save master dataset\n",
    "all_data.to_csv('MASTER_DATASET.csv', index=False)\n",
    "print(f\"\\nüíæ Created MASTER_DATASET.csv: {len(all_data)} segments\")\n",
    "print(f\"   Stable: {(all_data['deterioration']==0).sum()}\")\n",
    "print(f\"   Deteriorating: {(all_data['deterioration']==1).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5127795d-2a78-4c45-927f-e610972253ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded: 91 segments\n",
      "   Stable: 65\n",
      "   Deteriorating: 26\n",
      "   Deterioration rate: 28.6%\n",
      "\n",
      "üîç Checking signals in all 91 segments...\n",
      "  Checked 20/91...\n",
      "  Checked 40/91...\n",
      "  Checked 60/91...\n",
      "  Checked 80/91...\n",
      "\n",
      "üìä Signal Availability Across Your 91 Segments:\n",
      "============================================================\n",
      "  ABP            :  91/91 segments (100.0%)\n",
      "  II             :  90/91 segments (98.9%)\n",
      "  PLETH          :  47/91 segments (51.6%)\n",
      "  RESP           :  42/91 segments (46.2%)\n",
      "  V              :  35/91 segments (38.5%)\n",
      "  AVR            :  15/91 segments (16.5%)\n",
      "  III            :  11/91 segments (12.1%)\n",
      "  CVP            :   9/91 segments (9.9%)\n",
      "  PAP            :   7/91 segments (7.7%)\n",
      "  AVF            :   3/91 segments (3.3%)\n",
      "  ICP            :   1/91 segments (1.1%)\n",
      "\n",
      "üéØ Checking for Professor's Required Signals:\n",
      "  ‚úÖ Heart Rate (ECG): ‚úÖ YES\n",
      "  ‚úÖ Blood Pressure (ABP): ‚úÖ YES\n",
      "  ‚ö†Ô∏è  Respiration (RESP): ‚úÖ YES\n",
      "  ‚ö†Ô∏è  Temperature (TEMP): ‚ùå NO\n",
      "  ‚ö†Ô∏è  Oxygen (SpO2/PLETH): ‚úÖ YES\n"
     ]
    }
   ],
   "source": [
    "# Load the master dataset\n",
    "all_data = pd.read_csv('MASTER_DATASET.csv')\n",
    "\n",
    "print(f\"‚úÖ Loaded: {len(all_data)} segments\")\n",
    "print(f\"   Stable: {(all_data['deterioration']==0).sum()}\")\n",
    "print(f\"   Deteriorating: {(all_data['deterioration']==1).sum()}\")\n",
    "print(f\"   Deterioration rate: {(all_data['deterioration'].mean())*100:.1f}%\")\n",
    "\n",
    "# Check what signals are available\n",
    "print(f\"\\nüîç Checking signals in all {len(all_data)} segments...\")\n",
    "\n",
    "signal_counts = {}\n",
    "\n",
    "for i, row in all_data.iterrows():\n",
    "    patient = row['patient_id']\n",
    "    segment = row['segment']\n",
    "    \n",
    "    try:\n",
    "        folder, patient_id = patient.split('/')\n",
    "        record = wfdb.rdrecord(f\"data/{folder}/{patient_id}/{segment}\")\n",
    "        \n",
    "        for sig in record.sig_name:\n",
    "            signal_counts[sig] = signal_counts.get(sig, 0) + 1\n",
    "            \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if (i+1) % 20 == 0:\n",
    "        print(f\"  Checked {i+1}/{len(all_data)}...\")\n",
    "\n",
    "print(f\"\\nüìä Signal Availability Across Your {len(all_data)} Segments:\")\n",
    "print(\"=\"*60)\n",
    "for sig, count in sorted(signal_counts.items(), key=lambda x: -x[1]):\n",
    "    pct = (count/len(all_data))*100\n",
    "    print(f\"  {sig:15s}: {count:3d}/{len(all_data)} segments ({pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüéØ Checking for Professor's Required Signals:\")\n",
    "print(f\"  ‚úÖ Heart Rate (ECG): {'‚úÖ YES' if any('II' in s or 'ECG' in s for s in signal_counts.keys()) else '‚ùå NO'}\")\n",
    "print(f\"  ‚úÖ Blood Pressure (ABP): {'‚úÖ YES' if 'ABP' in signal_counts else '‚ùå NO'}\")\n",
    "print(f\"  ‚ö†Ô∏è  Respiration (RESP): {'‚úÖ YES' if 'RESP' in signal_counts else '‚ùå NO'}\")\n",
    "print(f\"  ‚ö†Ô∏è  Temperature (TEMP): {'‚úÖ YES' if 'TEMP' in signal_counts else '‚ùå NO'}\")\n",
    "print(f\"  ‚ö†Ô∏è  Oxygen (SpO2/PLETH): {'‚úÖ YES' if any(s in signal_counts for s in ['SpO2', 'PLETH']) else '‚ùå NO'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49c71287-6095-4837-b66d-91a55e62e94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Searching 2,273 patients for segments with COMPLETE vital signs...\n",
      "Required: ECG + ABP + RESP + PLETH/SpO2 + >= 4 hours\n",
      "\n",
      "‚úÖ #1: 30/3000393/3000393_0005 (4.8h)\n",
      "‚úÖ #2: 30/3000393/3000393_0008 (4.1h)\n",
      "‚úÖ #3: 30/3000393/3000393_0020 (5.2h)\n",
      "‚úÖ #4: 30/3000480/3000480_0018 (18.0h)\n",
      "‚úÖ #5: 30/3000714/3000714_0012 (8.3h)\n",
      "‚úÖ #6: 30/3000714/3000714_0035 (7.3h)\n",
      "‚úÖ #7: 30/3000781/3000781_0005 (6.9h)\n",
      "‚úÖ #8: 30/3000860/3000860_0007 (7.7h)\n",
      "Scanned 50 patients... Found 8 complete segments\n",
      "‚úÖ #9: 30/3000989/3000989_0002 (28.0h)\n",
      "‚úÖ #10: 30/3000989/3000989_0004 (14.4h)\n",
      "‚úÖ #11: 30/3000989/3000989_0006 (16.2h)\n",
      "‚úÖ #12: 30/3000989/3000989_0012 (17.6h)\n",
      "‚úÖ #13: 30/3000989/3000989_0017 (7.2h)\n",
      "‚úÖ #14: 30/3000989/3000989_0019 (10.4h)\n",
      "‚úÖ #15: 30/3000989/3000989_0022 (13.3h)\n",
      "‚úÖ #16: 30/3000989/3000989_0025 (23.0h)\n",
      "‚úÖ #17: 30/3000989/3000989_0028 (11.2h)\n",
      "‚úÖ #18: 30/3001203/3001203_0015 (9.1h)\n",
      "‚úÖ #19: 30/3001203/3001203_0018 (10.4h)\n",
      "‚úÖ #20: 30/3001203/3001203_0044 (6.1h)\n",
      "‚úÖ #21: 30/3001203/3001203_0050 (5.0h)\n",
      "‚úÖ #22: 30/3001554/3001554_0006 (21.8h)\n",
      "Scanned 100 patients... Found 22 complete segments\n",
      "‚úÖ #23: 30/3001937/3001937_0002 (6.0h)\n",
      "‚úÖ #24: 30/3001937/3001937_0009 (20.3h)\n",
      "‚úÖ #25: 30/3002094/3002094_0012 (10.4h)\n",
      "‚úÖ #26: 30/3002094/3002094_0021 (4.9h)\n",
      "‚úÖ #27: 30/3002151/3002151_0005 (13.9h)\n",
      "Scanned 150 patients... Found 27 complete segments\n",
      "‚úÖ #28: 31/3100038/3100038_0101 (9.1h)\n",
      "‚úÖ #29: 31/3100038/3100038_0107 (4.9h)\n",
      "‚úÖ #30: 31/3100038/3100038_0113 (21.4h)\n",
      "‚úÖ #31: 31/3100038/3100038_0117 (11.4h)\n",
      "‚úÖ #32: 31/3100038/3100038_0120 (28.5h)\n",
      "‚úÖ #33: 31/3100038/3100038_0132 (25.7h)\n",
      "‚úÖ #34: 31/3100038/3100038_0134 (4.2h)\n",
      "‚úÖ #35: 31/3100038/3100038_0138 (8.8h)\n",
      "‚úÖ #36: 31/3100038/3100038_0143 (5.0h)\n",
      "‚úÖ #37: 31/3100038/3100038_0154 (12.7h)\n",
      "‚úÖ #38: 31/3100038/3100038_0157 (15.9h)\n",
      "‚úÖ #39: 31/3100331/3100331_0006 (4.6h)\n",
      "‚úÖ #40: 31/3100331/3100331_0008 (16.0h)\n",
      "Scanned 200 patients... Found 40 complete segments\n",
      "‚úÖ #41: 31/3100835/3100835_0004 (11.2h)\n",
      "‚úÖ #42: 31/3101354/3101354_0004 (16.2h)\n",
      "‚úÖ #43: 31/3101412/3101412_0001 (14.0h)\n",
      "‚úÖ #44: 31/3101412/3101412_0003 (24.6h)\n",
      "‚úÖ #45: 31/3101488/3101488_0011 (25.5h)\n",
      "‚úÖ #46: 31/3101488/3101488_0015 (4.1h)\n",
      "‚úÖ #47: 31/3101488/3101488_0019 (7.6h)\n",
      "Scanned 250 patients... Found 47 complete segments\n",
      "‚úÖ #48: 31/3101871/3101871_0012 (9.2h)\n",
      "‚úÖ #49: 31/3102026/3102026_0005 (6.0h)\n",
      "‚úÖ #50: 31/3102282/3102282_0011 (7.2h)\n",
      "‚úÖ #51: 31/3102282/3102282_0013 (8.9h)\n",
      "‚úÖ #52: 31/3102401/3102401_0017 (4.3h)\n",
      "‚úÖ #53: 31/3102401/3102401_0020 (6.2h)\n",
      "‚úÖ #54: 31/3102401/3102401_0031 (6.5h)\n",
      "‚úÖ #55: 31/3102401/3102401_0045 (4.8h)\n",
      "‚úÖ #56: 31/3102401/3102401_0059 (6.3h)\n",
      "‚úÖ #57: 31/3102401/3102401_0072 (6.0h)\n",
      "‚úÖ #58: 31/3102401/3102401_0075 (6.3h)\n",
      "‚úÖ #59: 31/3102401/3102401_0078 (4.4h)\n",
      "‚úÖ #60: 31/3102401/3102401_0081 (7.3h)\n",
      "Scanned 300 patients... Found 60 complete segments\n",
      "‚úÖ #61: 31/3102651/3102651_0015 (5.2h)\n",
      "‚úÖ #62: 31/3102779/3102779_0003 (10.2h)\n",
      "‚úÖ #63: 31/3102779/3102779_0012 (23.7h)\n",
      "‚úÖ #64: 31/3102912/3102912_0016 (11.3h)\n",
      "‚úÖ #65: 31/3102912/3102912_0018 (5.1h)\n",
      "‚úÖ #66: 31/3102912/3102912_0021 (5.1h)\n",
      "‚úÖ #67: 31/3102912/3102912_0024 (11.1h)\n",
      "‚úÖ #68: 31/3102930/3102930_0004 (9.3h)\n",
      "‚úÖ #69: 31/3102930/3102930_0008 (7.2h)\n",
      "‚úÖ #70: 31/3102930/3102930_0011 (5.0h)\n",
      "‚úÖ #71: 31/3102930/3102930_0019 (4.3h)\n",
      "‚úÖ #72: 31/3102930/3102930_0030 (8.8h)\n",
      "‚úÖ #73: 31/3102930/3102930_0043 (8.1h)\n",
      "‚úÖ #74: 31/3102930/3102930_0046 (11.0h)\n",
      "‚úÖ #75: 31/3102930/3102930_0060 (5.3h)\n",
      "‚úÖ #76: 31/3102930/3102930_0066 (5.2h)\n",
      "‚úÖ #77: 31/3102930/3102930_0088 (8.9h)\n",
      "‚úÖ #78: 31/3102930/3102930_0091 (13.1h)\n",
      "‚úÖ #79: 31/3102930/3102930_0094 (4.9h)\n",
      "‚úÖ #80: 31/3103105/3103105_0002 (4.4h)\n",
      "‚úÖ #81: 31/3103105/3103105_0006 (17.5h)\n",
      "‚úÖ #82: 31/3103105/3103105_0019 (4.1h)\n",
      "Scanned 350 patients... Found 82 complete segments\n",
      "‚úÖ #83: 31/3103410/3103410_0031 (9.5h)\n",
      "‚úÖ #84: 31/3103413/3103413_0006 (8.5h)\n",
      "‚úÖ #85: 31/3103413/3103413_0016 (8.3h)\n",
      "‚úÖ #86: 31/3103413/3103413_0020 (29.3h)\n",
      "‚úÖ #87: 31/3103807/3103807_0003 (4.6h)\n",
      "‚úÖ #88: 31/3103807/3103807_0009 (4.4h)\n",
      "‚úÖ #89: 31/3103807/3103807_0011 (9.3h)\n",
      "‚úÖ #90: 31/3103807/3103807_0014 (14.3h)\n",
      "‚úÖ #91: 31/3103807/3103807_0021 (10.8h)\n",
      "‚úÖ #92: 31/3103807/3103807_0024 (29.5h)\n",
      "‚úÖ #93: 31/3103807/3103807_0030 (13.9h)\n",
      "‚úÖ #94: 31/3103807/3103807_0032 (7.3h)\n",
      "‚úÖ #95: 31/3103807/3103807_0039 (26.4h)\n",
      "‚úÖ #96: 31/3103807/3103807_0043 (13.9h)\n",
      "‚úÖ #97: 31/3103807/3103807_0051 (4.5h)\n",
      "‚úÖ #98: 31/3103807/3103807_0052 (5.7h)\n",
      "‚úÖ #99: 31/3103807/3103807_0058 (17.4h)\n",
      "‚úÖ #100: 31/3103807/3103807_0063 (7.7h)\n",
      "\n",
      "üéØ Target reached! Found 100 complete segments\n",
      "\n",
      "‚úÖ Search complete!\n",
      "   Found 100 segments with ALL required signals\n"
     ]
    }
   ],
   "source": [
    "# Search for segments with complete vital signs\n",
    "print(\"üîç Searching 2,273 patients for segments with COMPLETE vital signs...\")\n",
    "print(\"Required: ECG + ABP + RESP + PLETH/SpO2 + >= 4 hours\\n\")\n",
    "\n",
    "complete_segments = []\n",
    "\n",
    "for i, patient in enumerate(downloaded[:500]):  # Start with first 500 patients\n",
    "    folder, patient_id = patient.split('/')\n",
    "    patient_path = f'data/{folder}/{patient_id}'\n",
    "    \n",
    "    try:\n",
    "        files = os.listdir(patient_path)\n",
    "        segment_files = [f.replace('.hea', '') for f in files \n",
    "                        if f.endswith('.hea') and '_' in f and 'layout' not in f]\n",
    "        \n",
    "        for seg_name in segment_files:\n",
    "            try:\n",
    "                record = wfdb.rdrecord(f'{patient_path}/{seg_name}')\n",
    "                duration_hrs = record.sig_len / record.fs / 3600\n",
    "                \n",
    "                # Check for ALL required signals\n",
    "                has_ecg = any('II' in s or 'ECG' in s for s in record.sig_name)\n",
    "                has_abp = any('ABP' in s or 'BP' in s for s in record.sig_name)\n",
    "                has_resp = any('RESP' in s for s in record.sig_name)\n",
    "                has_spo2 = any('PLETH' in s or 'SpO2' in s for s in record.sig_name)\n",
    "                \n",
    "                # Check duration\n",
    "                if duration_hrs >= 4 and has_ecg and has_abp and has_resp and has_spo2:\n",
    "                    complete_segments.append({\n",
    "                        'patient': patient,\n",
    "                        'segment': seg_name,\n",
    "                        'duration': duration_hrs,\n",
    "                        'signals': record.sig_name\n",
    "                    })\n",
    "                    print(f\"‚úÖ #{len(complete_segments)}: {patient}/{seg_name} ({duration_hrs:.1f}h)\")\n",
    "                    \n",
    "                    # Stop if we found enough\n",
    "                    if len(complete_segments) >= 100:\n",
    "                        print(f\"\\nüéØ Target reached! Found {len(complete_segments)} complete segments\")\n",
    "                        break\n",
    "                        \n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "        if len(complete_segments) >= 100:\n",
    "            break\n",
    "            \n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f\"Scanned {i + 1} patients... Found {len(complete_segments)} complete segments\")\n",
    "            \n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(f\"\\n‚úÖ Search complete!\")\n",
    "print(f\"   Found {len(complete_segments)} segments with ALL required signals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "864083c9-4e96-4bf8-a93f-7c30e8b7f12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved 100 complete segments\n",
      "\n",
      "üìä Preview:\n",
      "      patient       segment   duration                              signals\n",
      "0  30/3000393  3000393_0005   4.813056       [ABP, RESP, PLETH, II, V, AVR]\n",
      "1  30/3000393  3000393_0008   4.051389       [ABP, RESP, PLETH, II, V, AVR]\n",
      "2  30/3000393  3000393_0020   5.158333       [ABP, RESP, PLETH, II, V, AVR]\n",
      "3  30/3000480  3000480_0018  18.029722       [ABP, RESP, V, III, II, PLETH]\n",
      "4  30/3000714  3000714_0012   8.267222  [PLETH, RESP, ABP, II, V, AVR, CVP]\n",
      "5  30/3000714  3000714_0035   7.296111  [PLETH, RESP, ABP, II, V, AVR, CVP]\n",
      "6  30/3000781  3000781_0005   6.898056       [PLETH, RESP, II, V, AVR, ABP]\n",
      "7  30/3000860  3000860_0007   7.743333       [RESP, PLETH, II, V, AVR, ABP]\n",
      "8  30/3000989  3000989_0002  27.978056       [RESP, ABP, II, III, V, PLETH]\n",
      "9  30/3000989  3000989_0004  14.375000       [RESP, ABP, II, III, V, PLETH]\n"
     ]
    }
   ],
   "source": [
    "# Save complete segments list\n",
    "complete_df = pd.DataFrame(complete_segments)\n",
    "complete_df.to_csv('complete_segments_with_all_vitals.csv', index=False)\n",
    "\n",
    "print(f\"üíæ Saved {len(complete_segments)} complete segments\")\n",
    "print(f\"\\nüìä Preview:\")\n",
    "print(complete_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078ced7a-a8bb-4154-bf67-6be66268d6de",
   "metadata": {},
   "source": [
    "# Complete Feature Extraction with ALL Vital Signs\n",
    "\n",
    "Now we extract features from ECG, ABP, RESP, and PLETH to meet professor's requirements.\n",
    "\n",
    "## Step 18: Enhanced Feature Extraction Function\n",
    "\n",
    "Extract features from all 4 vital sign categories:\n",
    "1. Heart Rate (from ECG)\n",
    "2. Blood Pressure (from ABP)\n",
    "3. Respiration Rate (from RESP)\n",
    "4. Oxygen Saturation (from PLETH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dcdbc34b-4f06-4c7f-8b6e-7616ab238892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fixed function defined!\n"
     ]
    }
   ],
   "source": [
    "def extract_complete_features(patient, segment_name):\n",
    "    \"\"\"Extract features from ALL vital signs including RESP and SpO2\"\"\"\n",
    "    \n",
    "    # Load record\n",
    "    folder, patient_id = patient.split('/')\n",
    "    record = wfdb.rdrecord(f\"data/{folder}/{patient_id}/{segment_name}\")\n",
    "    \n",
    "    # Find signal indices\n",
    "    ecg_idx = None\n",
    "    abp_idx = None\n",
    "    resp_idx = None\n",
    "    pleth_idx = None\n",
    "    \n",
    "    for i, name in enumerate(record.sig_name):\n",
    "        if 'II' in name or 'ECG' in name:\n",
    "            ecg_idx = i\n",
    "        if 'ABP' in name or 'BP' in name:\n",
    "            abp_idx = i\n",
    "        if 'RESP' in name:\n",
    "            resp_idx = i\n",
    "        if 'PLETH' in name or 'SpO2' in name:\n",
    "            pleth_idx = i\n",
    "    \n",
    "    if ecg_idx is None or abp_idx is None or resp_idx is None or pleth_idx is None:\n",
    "        raise ValueError(f\"Missing required signals\")\n",
    "    \n",
    "    # Get signals\n",
    "    ecg = record.p_signal[:, ecg_idx]\n",
    "    abp = record.p_signal[:, abp_idx]\n",
    "    resp = record.p_signal[:, resp_idx]\n",
    "    pleth = record.p_signal[:, pleth_idx]\n",
    "    \n",
    "    # === HEART RATE ===\n",
    "    peaks_ecg, _ = find_peaks(ecg, distance=record.fs*0.5, height=0.3)\n",
    "    if len(peaks_ecg) < 10:\n",
    "        raise ValueError(f\"Too few heartbeats\")\n",
    "    rr_intervals = np.diff(peaks_ecg) / record.fs\n",
    "    heart_rates = 60 / rr_intervals\n",
    "    \n",
    "    # === BLOOD PRESSURE ===\n",
    "    systolic_peaks, _ = find_peaks(abp, distance=record.fs*0.5, height=50)\n",
    "    if len(systolic_peaks) < 10:\n",
    "        raise ValueError(f\"Too few BP peaks\")\n",
    "    systolic_values = abp[systolic_peaks]\n",
    "    \n",
    "    # === RESPIRATION RATE (FIXED!) ===\n",
    "    resp_peaks, _ = find_peaks(resp, distance=record.fs*2, height=np.mean(resp))\n",
    "    if len(resp_peaks) < 5:\n",
    "        raise ValueError(f\"Too few respiratory cycles\")\n",
    "    resp_intervals = np.diff(resp_peaks) / record.fs  # seconds between breaths\n",
    "    resp_rates = 60 / resp_intervals  # breaths per minute\n",
    "    \n",
    "    # === OXYGEN SATURATION ===\n",
    "    pleth_peaks, _ = find_peaks(pleth, distance=record.fs*0.5)\n",
    "    if len(pleth_peaks) > 10:\n",
    "        pleth_amplitudes = pleth[pleth_peaks]\n",
    "        pleth_quality = np.std(pleth_amplitudes)\n",
    "    else:\n",
    "        pleth_quality = 0\n",
    "    \n",
    "    # Calculate trends\n",
    "    midpoint_hr = len(heart_rates) // 2\n",
    "    midpoint_bp = len(systolic_values) // 2\n",
    "    midpoint_resp = len(resp_rates) // 2\n",
    "    \n",
    "    hr_early = np.mean(heart_rates[:midpoint_hr])\n",
    "    hr_late = np.mean(heart_rates[midpoint_hr:])\n",
    "    \n",
    "    bp_early = np.mean(systolic_values[:midpoint_bp])\n",
    "    bp_late = np.mean(systolic_values[midpoint_bp:])\n",
    "    \n",
    "    resp_early = np.mean(resp_rates[:midpoint_resp])\n",
    "    resp_late = np.mean(resp_rates[midpoint_resp:])\n",
    "    \n",
    "    # Create features\n",
    "    features = {\n",
    "        'patient_id': patient,\n",
    "        'segment': segment_name,\n",
    "        'duration_hours': record.sig_len / record.fs / 3600,\n",
    "        \n",
    "        'hr_mean': np.mean(heart_rates),\n",
    "        'hr_std': np.std(heart_rates),\n",
    "        'hr_min': np.min(heart_rates),\n",
    "        'hr_max': np.max(heart_rates),\n",
    "        'hr_early_mean': hr_early,\n",
    "        'hr_late_mean': hr_late,\n",
    "        'hr_change': hr_late - hr_early,\n",
    "        'hr_percent_change': ((hr_late - hr_early) / hr_early) * 100,\n",
    "        \n",
    "        'bp_systolic_mean': np.mean(systolic_values),\n",
    "        'bp_systolic_std': np.std(systolic_values),\n",
    "        'bp_systolic_min': np.min(systolic_values),\n",
    "        'bp_systolic_max': np.max(systolic_values),\n",
    "        'bp_early_mean': bp_early,\n",
    "        'bp_late_mean': bp_late,\n",
    "        'bp_change': bp_late - bp_early,\n",
    "        'bp_percent_change': ((bp_late - bp_early) / bp_early) * 100,\n",
    "        \n",
    "        'resp_rate_mean': np.mean(resp_rates),\n",
    "        'resp_rate_std': np.std(resp_rates),\n",
    "        'resp_rate_min': np.min(resp_rates),\n",
    "        'resp_rate_max': np.max(resp_rates),\n",
    "        'resp_early_mean': resp_early,\n",
    "        'resp_late_mean': resp_late,\n",
    "        'resp_change': resp_late - resp_early,\n",
    "        'resp_percent_change': ((resp_late - resp_early) / resp_early) * 100,\n",
    "        \n",
    "        'pleth_quality': pleth_quality,\n",
    "        'pleth_mean': np.mean(pleth),\n",
    "        'pleth_std': np.std(pleth),\n",
    "        \n",
    "        'shock_index': np.mean(heart_rates) / np.mean(systolic_values)\n",
    "    }\n",
    "    \n",
    "    return features\n",
    "\n",
    "print(\"‚úÖ Fixed function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8327a1b1-e07e-4a7d-b6d0-0fe092dc4043",
   "metadata": {},
   "source": [
    "## Step 19: Test Feature Extraction Function\n",
    "\n",
    "Before processing all 100 segments, we test the enhanced feature extraction function on ONE patient to verify it works correctly.\n",
    "\n",
    "**What this code does:**\n",
    "1. Loads the list of 100 complete segments (with all vital signs)\n",
    "2. Selects the first patient as a test case\n",
    "3. Runs the `extract_complete_features()` function\n",
    "4. Displays sample features to verify output\n",
    "\n",
    "**What we're checking:**\n",
    "- ‚úÖ All 31 features are extracted correctly\n",
    "- ‚úÖ Heart Rate, Blood Pressure, Respiration, and PLETH values are reasonable\n",
    "- ‚úÖ No errors occur\n",
    "- ‚úÖ Feature values make clinical sense\n",
    "\n",
    "**Expected output:**\n",
    "- Feature count: 31 features\n",
    "- Sample values showing HR (~73 bpm), BP (~153 mmHg), Respiration (~21 br/min), PLETH quality (~0.33), Shock Index (~0.48)\n",
    "\n",
    "**Why this is important:**\n",
    "Testing on one patient before processing all 100 saves time if there are bugs. If this works, we know the function is ready for batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2aaafa84-6dba-445d-9983-34eefe3088f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing enhanced feature extraction with ALL vitals...\n",
      "‚úÖ Loaded 100 complete segments\n",
      "\n",
      "Testing: 30/3000393/3000393_0005\n",
      "\n",
      "‚úÖ SUCCESS! Extracted 31 features\n",
      "\n",
      "üìä Sample features:\n",
      "  Heart Rate: 73.4 bpm (change: +10.6)\n",
      "  Blood Pressure: 153.3 mmHg (change: +2.2)\n",
      "  Respiration: 20.8 breaths/min (change: -2.4)\n",
      "  PLETH Quality: 0.33\n",
      "  Shock Index: 0.48\n"
     ]
    }
   ],
   "source": [
    "# Test enhanced feature extraction on first complete segment\n",
    "print(\"üß™ Testing enhanced feature extraction with ALL vitals...\")\n",
    "\n",
    "# Load complete segments\n",
    "complete_df = pd.read_csv('complete_segments_with_all_vitals.csv')\n",
    "print(f\"‚úÖ Loaded {len(complete_df)} complete segments\\n\")\n",
    "\n",
    "# Test on first segment\n",
    "test_seg = complete_df.iloc[0]\n",
    "print(f\"Testing: {test_seg['patient']}/{test_seg['segment']}\")\n",
    "\n",
    "try:\n",
    "    test_features = extract_complete_features(test_seg['patient'], test_seg['segment'])\n",
    "    \n",
    "    print(f\"\\n‚úÖ SUCCESS! Extracted {len(test_features)} features\")\n",
    "    print(f\"\\nüìä Sample features:\")\n",
    "    print(f\"  Heart Rate: {test_features['hr_mean']:.1f} bpm (change: {test_features['hr_change']:+.1f})\")\n",
    "    print(f\"  Blood Pressure: {test_features['bp_systolic_mean']:.1f} mmHg (change: {test_features['bp_change']:+.1f})\")\n",
    "    print(f\"  Respiration: {test_features['resp_rate_mean']:.1f} breaths/min (change: {test_features['resp_change']:+.1f})\")\n",
    "    print(f\"  PLETH Quality: {test_features['pleth_quality']:.2f}\")\n",
    "    print(f\"  Shock Index: {test_features['shock_index']:.2f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR: {e}\")\n",
    "    print(\"\\nLet's debug this!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51eb8c1-ba1e-4410-8e5b-09223d7cf2d8",
   "metadata": {},
   "source": [
    "## Step 20: Extract Features from All 100 Complete Segments\n",
    "\n",
    "Now we process all 100 segments that have complete vital signs (ECG, ABP, RESP, PLETH).\n",
    "\n",
    "**What this code does:**\n",
    "1. Loops through all 100 complete segments\n",
    "2. For each segment, calls `extract_complete_features()` to extract 31 features\n",
    "3. Handles errors gracefully (some segments may have poor signal quality)\n",
    "4. Shows progress every 10 segments\n",
    "5. Converts results to DataFrame and saves to CSV\n",
    "\n",
    "**Processing time:** 5-10 minutes\n",
    "\n",
    "**Expected results:**\n",
    "- Successfully extract features from ~90-95 segments\n",
    "- 5-10 segments may fail due to:\n",
    "  - Too few respiratory cycles detected (most common error)\n",
    "  - Too few heartbeats or BP peaks detected\n",
    "  - Poor signal quality or sensor disconnections\n",
    "\n",
    "**Output file:** `complete_features_all_vitals.csv`\n",
    "- Contains: 31 features √ó ~91 segments\n",
    "- Includes: HR, BP, Respiration, PLETH/SpO2, duration, and clinical metrics (shock index)\n",
    "\n",
    "**Why some segments fail:**\n",
    "ICU waveform data is noisy. Some recordings have gaps, artifacts, or sensor disconnections. This is normal and expected in real-world medical data. We still get 91 high-quality segments, which is excellent for our analysis.\n",
    "\n",
    "**What happens next:**\n",
    "After this completes, we'll label each segment as \"stable\" or \"deteriorating\" based on clinical patterns in the vital signs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17be1d2b-9963-481c-a723-d2ed6036be32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Extracting features from all 100 complete segments...\n",
      "This will take 5-10 minutes...\n",
      "\n",
      "‚úÖ Processed 10/100 segments...\n",
      "‚ùå Error: 30/3000989/3000989_0006: Too few respiratory cycles\n",
      "‚úÖ Processed 20/100 segments...\n",
      "‚úÖ Processed 30/100 segments...\n",
      "‚úÖ Processed 40/100 segments...\n",
      "‚ùå Error: 31/3101412/3101412_0001: Too few respiratory cycles\n",
      "‚ùå Error: 31/3101412/3101412_0003: Too few respiratory cycles\n",
      "‚úÖ Processed 50/100 segments...\n",
      "‚úÖ Processed 60/100 segments...\n",
      "‚ùå Error: 31/3102651/3102651_0015: Too few BP peaks\n",
      "‚ùå Error: 31/3102779/3102779_0012: Too few respiratory cycles\n",
      "‚ùå Error: 31/3102912/3102912_0016: Too few respiratory cycles\n",
      "‚úÖ Processed 70/100 segments...\n",
      "‚ùå Error: 31/3103105/3103105_0002: Too few respiratory cycles\n",
      "‚ùå Error: 31/3103807/3103807_0014: Too few respiratory cycles\n",
      "‚ùå Error: 31/3103807/3103807_0021: Too few respiratory cycles\n",
      "‚úÖ Processed 100/100 segments...\n",
      "\n",
      "üéâ COMPLETE!\n",
      "   Successfully extracted: 91/100 segments\n",
      "   Errors: 9 segments failed\n",
      "\n",
      "üíæ Saved to: complete_features_all_vitals.csv\n",
      "   Dataset: 91 segments √ó 31 features\n",
      "\n",
      "üìä Preview:\n",
      "   patient_id       segment  duration_hours    hr_mean     hr_std     hr_min  \\\n",
      "0  30/3000393  3000393_0005        4.813056  73.357148   8.054186  51.020408   \n",
      "1  30/3000393  3000393_0008        4.051389  74.753314   8.310013  42.134831   \n",
      "2  30/3000393  3000393_0020        5.158333  70.761289   7.429376  33.936652   \n",
      "3  30/3000480  3000480_0018       18.029722  87.323933   8.212471   1.613250   \n",
      "4  30/3000714  3000714_0012        8.267222  85.826180  23.989346  33.783784   \n",
      "\n",
      "       hr_max  hr_early_mean  hr_late_mean  hr_change  ...  resp_rate_min  \\\n",
      "0  119.047619      68.059960     78.654336  10.594376  ...       4.288165   \n",
      "1  119.047619      74.669811     74.836807   0.166996  ...       2.277558   \n",
      "2  119.047619      67.360000     74.162265   6.802265  ...       4.895561   \n",
      "3  119.047619      91.655744     82.992214  -8.663530  ...       0.847362   \n",
      "4  119.047619      85.641552     86.010809   0.369257  ...       3.797468   \n",
      "\n",
      "   resp_rate_max  resp_early_mean  resp_late_mean  resp_change  \\\n",
      "0           30.0        21.952117       19.601843    -2.350273   \n",
      "1           30.0        19.812168       19.914717     0.102549   \n",
      "2           30.0        17.824638       17.935317     0.110679   \n",
      "3           30.0        15.855774       14.806907    -1.048867   \n",
      "4           30.0        16.069482       15.840936    -0.228546   \n",
      "\n",
      "   resp_percent_change  pleth_quality  pleth_mean  pleth_std  shock_index  \n",
      "0           -10.706364       0.333377    1.829289   0.468703     0.478391  \n",
      "1             0.517607       0.361844    1.911041   0.431027     0.524744  \n",
      "2             0.620933       0.431512    1.894886   0.660868     0.529512  \n",
      "3            -6.615047       0.368337    1.844309   0.551393     0.784770  \n",
      "4            -1.422237       0.069023         NaN        NaN     0.882155  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"üöÄ Extracting features from all 100 complete segments...\")\n",
    "print(\"This will take 5-10 minutes...\\n\")\n",
    "\n",
    "all_complete_features = []\n",
    "errors = []\n",
    "\n",
    "for i, row in complete_df.iterrows():\n",
    "    try:\n",
    "        features = extract_complete_features(row['patient'], row['segment'])\n",
    "        all_complete_features.append(features)\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"‚úÖ Processed {i + 1}/100 segments...\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        errors.append({'patient': row['patient'], 'segment': row['segment'], 'error': str(e)})\n",
    "        print(f\"‚ùå Error: {row['patient']}/{row['segment']}: {str(e)}\")\n",
    "\n",
    "print(f\"\\nüéâ COMPLETE!\")\n",
    "print(f\"   Successfully extracted: {len(all_complete_features)}/100 segments\")\n",
    "if errors:\n",
    "    print(f\"   Errors: {len(errors)} segments failed\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "complete_features_df = pd.DataFrame(all_complete_features)\n",
    "\n",
    "# Save\n",
    "complete_features_df.to_csv('complete_features_all_vitals.csv', index=False)\n",
    "print(f\"\\nüíæ Saved to: complete_features_all_vitals.csv\")\n",
    "print(f\"   Dataset: {len(complete_features_df)} segments √ó {len(complete_features_df.columns)} features\")\n",
    "\n",
    "print(f\"\\nüìä Preview:\")\n",
    "print(complete_features_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff1be2f-315c-4b11-b573-f7083dac2470",
   "metadata": {},
   "source": [
    "## Step 21: Label Segments as Stable or Deteriorating\n",
    "\n",
    "Now we create labels for our ML model by analyzing vital sign patterns to identify which segments show deterioration.\n",
    "\n",
    "**What is labeling?**\n",
    "We need to tell our ML model which segments represent \"deteriorating\" patients vs \"stable\" patients. We define clinical patterns that indicate deterioration based on established ICU early warning criteria.\n",
    "\n",
    "**Deterioration Patterns (label = 1):**\n",
    "1. **Classic compensatory shock:** HR increasing + BP decreasing\n",
    "2. **Tachycardia worsening:** High HR (>100 bpm) and rising\n",
    "3. **Hypotension worsening:** Low BP (<100 mmHg) and falling  \n",
    "4. **High shock index:** Ratio > 0.9 indicates hemodynamic instability\n",
    "5. **Large HR increase:** >20 bpm change\n",
    "6. **Large BP drop:** >15 mmHg decrease\n",
    "7. **Respiratory distress (NEW!):** Tachypnea (>25 br/min) and worsening\n",
    "8. **Bradypnea (NEW!):** Dangerously slow breathing (<8 br/min)\n",
    "\n",
    "**Stable Pattern (label = 0):**\n",
    "- No concerning patterns detected\n",
    "- Vital signs remain within normal ranges\n",
    "\n",
    "**What this code does:**\n",
    "1. Loads the extracted features from CSV\n",
    "2. Defines the `label_complete_segment()` function with 8 deterioration patterns\n",
    "3. Applies labeling to all 91 segments\n",
    "4. Calculates deterioration rate\n",
    "5. Saves labeled dataset to `FINAL_COMPLETE_DATASET.csv`\n",
    "6. Shows example segments from each category\n",
    "\n",
    "**Expected results:**\n",
    "- **Stable (0):** ~70 segments (77%)\n",
    "- **Deteriorating (1):** ~21 segments (23%)\n",
    "- **Deterioration rate:** ~23% (clinically realistic for ICU populations)\n",
    "\n",
    "**Why 23% is good:**\n",
    "In real ICU settings, 20-30% of patients show deterioration signs. Most patients are successfully stabilized, so our rate reflects actual clinical practice.\n",
    "\n",
    "**Output file:** `FINAL_COMPLETE_DATASET.csv`\n",
    "- All 31 features + deterioration label\n",
    "- Ready for ML training\n",
    "- 91 rows √ó 32 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b948bd4c-f2b0-45b4-b6bc-18df6a713ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Dataset: 91 segments with 31 features\n",
      "\n",
      "üìä Label Distribution:\n",
      "   Stable (0): 70 segments\n",
      "   Deteriorating (1): 21 segments\n",
      "   Deterioration rate: 23.1%\n",
      "\n",
      "üíæ Saved to: FINAL_COMPLETE_DATASET.csv\n",
      "\n",
      "‚úÖ Example STABLE segments:\n",
      "   patient_id    hr_mean  hr_change  bp_systolic_mean  bp_change  \\\n",
      "0  30/3000393  73.357148  10.594376        153.341455   2.170418   \n",
      "2  30/3000393  70.761289   6.802265        133.634807  17.892585   \n",
      "3  30/3000480  87.323933  -8.663530        111.273350   3.352389   \n",
      "\n",
      "   resp_rate_mean  resp_change  \n",
      "0       20.776781    -2.350273  \n",
      "2       17.879977     0.110679  \n",
      "3       15.331300    -1.048867  \n",
      "\n",
      "‚ö†Ô∏è  Example DETERIORATING segments:\n",
      "    patient_id    hr_mean  hr_change  bp_systolic_mean  bp_change  \\\n",
      "1   30/3000393  74.753314   0.166996        142.456722 -16.690726   \n",
      "7   30/3000860  82.754216  12.898548        119.935149 -10.600357   \n",
      "17  30/3001203  95.974713   5.298301        102.974386  -5.038004   \n",
      "\n",
      "    resp_rate_mean  resp_change  \n",
      "1        19.863442     0.102549  \n",
      "7        18.683412     4.554571  \n",
      "17       20.753311    -0.131452  \n"
     ]
    }
   ],
   "source": [
    "# Load the complete features\n",
    "complete_features_df = pd.read_csv('complete_features_all_vitals.csv')\n",
    "\n",
    "print(f\"üìä Dataset: {len(complete_features_df)} segments with {len(complete_features_df.columns)} features\")\n",
    "\n",
    "# Enhanced labeling function with respiratory rate\n",
    "def label_complete_segment(row):\n",
    "    \"\"\"\n",
    "    Label segment with ALL vital signs\n",
    "    Deterioration indicators:\n",
    "    - HR increasing + BP decreasing\n",
    "    - High HR + rising\n",
    "    - Low BP + falling\n",
    "    - High shock index\n",
    "    - Abnormal respiration patterns\n",
    "    \"\"\"\n",
    "    \n",
    "    # Pattern 1: Classic deterioration (HR up, BP down)\n",
    "    if row['hr_change'] > 10 and row['bp_change'] < -5:\n",
    "        return 1\n",
    "    \n",
    "    # Pattern 2: Tachycardia worsening\n",
    "    if row['hr_mean'] > 100 and row['hr_change'] > 5:\n",
    "        return 1\n",
    "    \n",
    "    # Pattern 3: Hypotension worsening\n",
    "    if row['bp_systolic_mean'] < 100 and row['bp_change'] < -5:\n",
    "        return 1\n",
    "    \n",
    "    # Pattern 4: High shock index\n",
    "    if row['shock_index'] > 0.9:\n",
    "        return 1\n",
    "    \n",
    "    # Pattern 5: Large HR increase\n",
    "    if row['hr_change'] > 20:\n",
    "        return 1\n",
    "    \n",
    "    # Pattern 6: Large BP drop\n",
    "    if row['bp_change'] < -15:\n",
    "        return 1\n",
    "    \n",
    "    # Pattern 7 (NEW!): Respiratory distress (tachypnea worsening)\n",
    "    if row['resp_rate_mean'] > 25 and row['resp_change'] > 3:\n",
    "        return 1\n",
    "    \n",
    "    # Pattern 8 (NEW!): Bradypnea (dangerously slow breathing)\n",
    "    if row['resp_rate_mean'] < 8:\n",
    "        return 1\n",
    "    \n",
    "    # Otherwise: Stable\n",
    "    return 0\n",
    "\n",
    "# Apply labeling\n",
    "complete_features_df['deterioration'] = complete_features_df.apply(label_complete_segment, axis=1)\n",
    "\n",
    "# Check distribution\n",
    "print(f\"\\nüìä Label Distribution:\")\n",
    "print(f\"   Stable (0): {(complete_features_df['deterioration']==0).sum()} segments\")\n",
    "print(f\"   Deteriorating (1): {(complete_features_df['deterioration']==1).sum()} segments\")\n",
    "print(f\"   Deterioration rate: {complete_features_df['deterioration'].mean()*100:.1f}%\")\n",
    "\n",
    "# Save labeled dataset\n",
    "complete_features_df.to_csv('FINAL_COMPLETE_DATASET.csv', index=False)\n",
    "print(f\"\\nüíæ Saved to: FINAL_COMPLETE_DATASET.csv\")\n",
    "\n",
    "# Show examples\n",
    "print(f\"\\n‚úÖ Example STABLE segments:\")\n",
    "print(complete_features_df[complete_features_df['deterioration']==0][\n",
    "    ['patient_id', 'hr_mean', 'hr_change', 'bp_systolic_mean', 'bp_change', 'resp_rate_mean', 'resp_change']\n",
    "].head(3))\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è  Example DETERIORATING segments:\")\n",
    "print(complete_features_df[complete_features_df['deterioration']==1][\n",
    "    ['patient_id', 'hr_mean', 'hr_change', 'bp_systolic_mean', 'bp_change', 'resp_rate_mean', 'resp_change']\n",
    "].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc04e9ca-49cf-4cfd-bfdc-34b340155c2d",
   "metadata": {},
   "source": [
    "## Step 22: Train Machine Learning Model\n",
    "\n",
    "Now we train a Random Forest classifier to predict patient deterioration based on vital sign features.\n",
    "\n",
    "**What is Random Forest?**\n",
    "An ensemble machine learning algorithm that creates multiple decision trees and combines their predictions. It's robust, handles imbalanced data well, and provides feature importance rankings.\n",
    "\n",
    "**Model Configuration:**\n",
    "- **Algorithm:** Random Forest Classifier\n",
    "- **Trees:** 100 decision trees\n",
    "- **Max depth:** 8 (prevents overfitting)\n",
    "- **Class weight:** Balanced (handles our 77% stable / 23% deteriorating imbalance)\n",
    "- **Random state:** 42 (for reproducibility)\n",
    "\n",
    "**What this code does:**\n",
    "1. Loads the labeled dataset (91 segments with 31 features + label)\n",
    "2. Separates features (X) from labels (y)\n",
    "3. Splits data into train (80%) and test (20%) sets\n",
    "4. Trains Random Forest model on training data\n",
    "5. Makes predictions on test set\n",
    "6. Evaluates performance with multiple metrics\n",
    "7. Identifies most important features\n",
    "8. Saves trained model for later use\n",
    "\n",
    "**Data Split:**\n",
    "- **Train set:** 72 segments (55 stable, 17 deteriorating)\n",
    "- **Test set:** 19 segments (15 stable, 4 deteriorating)\n",
    "- Uses stratified split to maintain class balance in both sets\n",
    "\n",
    "**Performance Metrics:**\n",
    "- **Accuracy:** Overall correctness (% of correct predictions)\n",
    "- **Precision:** Of patients flagged as deteriorating, how many actually are? (avoids false alarms)\n",
    "- **Recall:** Of actual deteriorating patients, how many did we catch? (sensitivity)\n",
    "- **Confusion Matrix:** Shows true positives, false positives, true negatives, false negatives\n",
    "\n",
    "**Expected Results:**\n",
    "- Accuracy: ~85-90%\n",
    "- Precision: ~80-100% (low false alarm rate)\n",
    "- Recall: ~50-75% (catches most deteriorating patients)\n",
    "- Trade-off: Conservative model that avoids false alarms\n",
    "\n",
    "**Feature Importance:**\n",
    "The model ranks which features are most predictive. Expected top features:\n",
    "1. Blood pressure changes (BP dropping = strong deterioration signal)\n",
    "2. Heart rate trends\n",
    "3. Shock index\n",
    "4. Respiratory changes\n",
    "\n",
    "**Output file:** `final_model_all_vitals.pkl`\n",
    "- Saved trained model\n",
    "- Can be loaded later to make predictions on new patients\n",
    "- Contains all learned patterns from training data\n",
    "\n",
    "**Clinical interpretation:**\n",
    "A high-precision model is preferred in healthcare to avoid \"alarm fatigue\" from false alarms, even if it means missing some cases (which triggers closer monitoring rather than missed diagnoses)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "16ef3314-9d80-454d-b6e5-56f21082eed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Training ML Model with Complete Vital Signs\n",
      "   Total segments: 91\n",
      "   Stable: 70\n",
      "   Deteriorating: 21\n",
      "\n",
      "üìã Using 29 features:\n",
      "   Heart Rate: 8 features\n",
      "   Blood Pressure: 8 features\n",
      "   Respiration: 8 features\n",
      "   PLETH/SpO2: 3 features\n",
      "   Clinical: 1 feature (shock index)\n",
      "   Duration: 1 feature\n",
      "\n",
      "üìö Train set: 72 (Stable: 55, Deteriorating: 17)\n",
      "üß™ Test set: 19 (Stable: 15, Deteriorating: 4)\n",
      "\n",
      "ü§ñ Training Random Forest...\n",
      "‚úÖ Model trained!\n",
      "\n",
      "üìà MODEL PERFORMANCE:\n",
      "   Accuracy: 89.5%\n",
      "   Precision: 100.0%\n",
      "   Recall: 50.0%\n",
      "\n",
      "üìä Confusion Matrix:\n",
      "[[15  0]\n",
      " [ 2  2]]\n",
      "\n",
      "True Negatives: 15, False Positives: 0\n",
      "False Negatives: 2, True Positives: 2\n",
      "\n",
      "üìã Detailed Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       Stable       0.88      1.00      0.94        15\n",
      "Deteriorating       1.00      0.50      0.67         4\n",
      "\n",
      "     accuracy                           0.89        19\n",
      "    macro avg       0.94      0.75      0.80        19\n",
      " weighted avg       0.91      0.89      0.88        19\n",
      "\n",
      "\n",
      "üîç Top 10 Most Important Features:\n",
      "          feature  importance\n",
      "        bp_change    0.136701\n",
      "bp_percent_change    0.119155\n",
      "     hr_late_mean    0.063607\n",
      "   resp_late_mean    0.055176\n",
      "hr_percent_change    0.051742\n",
      "       pleth_mean    0.048388\n",
      "     bp_late_mean    0.039701\n",
      "          hr_mean    0.037717\n",
      "      shock_index    0.037650\n",
      "   resp_rate_mean    0.037139\n",
      "\n",
      "üíæ Model saved to: final_model_all_vitals.pkl\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, classification_report\n",
    "\n",
    "# Load final dataset\n",
    "final_data = pd.read_csv('FINAL_COMPLETE_DATASET.csv')\n",
    "\n",
    "print(f\"üìä Training ML Model with Complete Vital Signs\")\n",
    "print(f\"   Total segments: {len(final_data)}\")\n",
    "print(f\"   Stable: {(final_data['deterioration']==0).sum()}\")\n",
    "print(f\"   Deteriorating: {(final_data['deterioration']==1).sum()}\")\n",
    "\n",
    "# Select feature columns (exclude patient_id, segment, deterioration)\n",
    "feature_columns = [col for col in final_data.columns \n",
    "                   if col not in ['patient_id', 'segment', 'deterioration']]\n",
    "\n",
    "print(f\"\\nüìã Using {len(feature_columns)} features:\")\n",
    "print(f\"   Heart Rate: 8 features\")\n",
    "print(f\"   Blood Pressure: 8 features\")\n",
    "print(f\"   Respiration: 8 features\")\n",
    "print(f\"   PLETH/SpO2: 3 features\")\n",
    "print(f\"   Clinical: 1 feature (shock index)\")\n",
    "print(f\"   Duration: 1 feature\")\n",
    "\n",
    "X = final_data[feature_columns]\n",
    "y = final_data['deterioration']\n",
    "\n",
    "# Split (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nüìö Train set: {len(X_train)} (Stable: {(y_train==0).sum()}, Deteriorating: {(y_train==1).sum()})\")\n",
    "print(f\"üß™ Test set: {len(X_test)} (Stable: {(y_test==0).sum()}, Deteriorating: {(y_test==1).sum()})\")\n",
    "\n",
    "# Train Random Forest\n",
    "print(f\"\\nü§ñ Training Random Forest...\")\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=8,\n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "print(f\"‚úÖ Model trained!\")\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluation\n",
    "print(f\"\\nüìà MODEL PERFORMANCE:\")\n",
    "print(f\"   Accuracy: {accuracy_score(y_test, y_pred):.1%}\")\n",
    "print(f\"   Precision: {precision_score(y_test, y_pred):.1%}\")\n",
    "print(f\"   Recall: {recall_score(y_test, y_pred):.1%}\")\n",
    "\n",
    "print(f\"\\nüìä Confusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "print(f\"\\nTrue Negatives: {cm[0,0]}, False Positives: {cm[0,1]}\")\n",
    "print(f\"False Negatives: {cm[1,0]}, True Positives: {cm[1,1]}\")\n",
    "\n",
    "print(f\"\\nüìã Detailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Stable', 'Deteriorating']))\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\nüîç Top 10 Most Important Features:\")\n",
    "print(feature_importance.head(10).to_string(index=False))\n",
    "\n",
    "# Save model\n",
    "import pickle\n",
    "with open('final_model_all_vitals.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "    \n",
    "print(f\"\\nüíæ Model saved to: final_model_all_vitals.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb390e5c-980e-4c03-bc3d-d72a300286e8",
   "metadata": {},
   "source": [
    "## Step 23: Test Complete System - ML Prediction on Sample Patient\n",
    "\n",
    "Before integrating the LLM, we test the trained ML model on one patient to verify it works correctly and see what data we'll send to the LLM.\n",
    "\n",
    "**What this code does:**\n",
    "1. Selects the first patient from our dataset as a test case\n",
    "2. Prepares their features for the ML model\n",
    "3. Gets ML prediction (stable vs deteriorating) and probability\n",
    "4. Displays comprehensive vital signs analysis\n",
    "5. Shows what information will be sent to the LLM for explanation\n",
    "\n",
    "**ML Model Output:**\n",
    "- **Classification:** STABLE or DETERIORATING (binary prediction)\n",
    "- **Probability:** Risk percentage (0-100%)\n",
    "- Higher probability = higher confidence in deterioration prediction\n",
    "\n",
    "**Vital Signs Display:**\n",
    "Shows the complete picture for clinical interpretation:\n",
    "- **Heart Rate:** Early vs Late values, absolute change, percent change\n",
    "- **Blood Pressure:** Early vs Late values, absolute change, percent change  \n",
    "- **Respiration Rate:** Early vs Late values, absolute change, percent change\n",
    "- **Shock Index:** Calculated ratio (HR/BP), with risk level indicator\n",
    "  - Normal: <0.7 ‚úì\n",
    "  - Elevated: 0.7-0.9 ‚ö°\n",
    "  - High: >0.9 ‚ö†Ô∏è (concerning!)\n",
    "\n",
    "**Why we test on one patient first:**\n",
    "- Verify ML model makes reasonable predictions\n",
    "- See actual vital sign values and trends\n",
    "- Understand what context the LLM will receive\n",
    "- Debug before processing all 91 patients\n",
    "\n",
    "**What happens next:**\n",
    "This vital signs summary will be formatted into a clinical prompt and sent to Claude API, which will generate a plain-English explanation of the patient's condition and deterioration risk.\n",
    "\n",
    "**Expected output:**\n",
    "- Patient identification\n",
    "- ML risk assessment  \n",
    "- Detailed vital signs breakdown\n",
    "- Confirmation that system is ready for LLM integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "81ad0a27-4e3f-4e40-a1a9-ac0ad92ad7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ COMPLETE SYSTEM TEST: ML + LLM\n",
      "======================================================================\n",
      "\n",
      "üë§ Patient: 30/3000393\n",
      "üìã Segment: 3000393_0005\n",
      "‚è±Ô∏è  Duration: 4.8 hours\n",
      "\n",
      "ü§ñ ML MODEL PREDICTION:\n",
      "   Risk Level: ‚úÖ STABLE\n",
      "   Probability: 11.0%\n",
      "\n",
      "üìä VITAL SIGNS ANALYSIS:\n",
      "\n",
      "  üíì Heart Rate:\n",
      "     Early: 68.1 bpm ‚Üí Late: 78.7 bpm\n",
      "     Change: +10.6 bpm (+15.6%)\n",
      "\n",
      "  ü©∫ Blood Pressure:\n",
      "     Early: 152.3 mmHg ‚Üí Late: 154.4 mmHg\n",
      "     Change: +2.2 mmHg (+1.4%)\n",
      "\n",
      "  ü´Å Respiration Rate:\n",
      "     Early: 22.0 br/min ‚Üí Late: 19.6 br/min\n",
      "     Change: -2.4 br/min (-10.7%)\n",
      "\n",
      "  üíâ Clinical Metrics:\n",
      "     Shock Index: 0.48 (‚úì Normal)\n",
      "\n",
      "üìù READY FOR LLM EXPLANATION!\n",
      "Next: We'll send this data to Claude API for clinical interpretation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\river\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\river\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Test the complete system: ML + LLM explanation\n",
    "\n",
    "# Select a test case\n",
    "test_idx = 0\n",
    "test_patient = final_data.iloc[test_idx]\n",
    "test_features_input = test_patient[feature_columns].values.reshape(1, -1)\n",
    "\n",
    "# ML Prediction\n",
    "prediction = model.predict(test_features_input)[0]\n",
    "probability = model.predict_proba(test_features_input)[0, 1]\n",
    "\n",
    "print(\"üî¨ COMPLETE SYSTEM TEST: ML + LLM\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüë§ Patient: {test_patient['patient_id']}\")\n",
    "print(f\"üìã Segment: {test_patient['segment']}\")\n",
    "print(f\"‚è±Ô∏è  Duration: {test_patient['duration_hours']:.1f} hours\")\n",
    "\n",
    "print(f\"\\nü§ñ ML MODEL PREDICTION:\")\n",
    "print(f\"   Risk Level: {'‚ö†Ô∏è DETERIORATING' if prediction == 1 else '‚úÖ STABLE'}\")\n",
    "print(f\"   Probability: {probability:.1%}\")\n",
    "\n",
    "print(f\"\\nüìä VITAL SIGNS ANALYSIS:\")\n",
    "print(f\"\\n  üíì Heart Rate:\")\n",
    "print(f\"     Early: {test_patient['hr_early_mean']:.1f} bpm ‚Üí Late: {test_patient['hr_late_mean']:.1f} bpm\")\n",
    "print(f\"     Change: {test_patient['hr_change']:+.1f} bpm ({test_patient['hr_percent_change']:+.1f}%)\")\n",
    "\n",
    "print(f\"\\n  ü©∫ Blood Pressure:\")\n",
    "print(f\"     Early: {test_patient['bp_early_mean']:.1f} mmHg ‚Üí Late: {test_patient['bp_late_mean']:.1f} mmHg\")\n",
    "print(f\"     Change: {test_patient['bp_change']:+.1f} mmHg ({test_patient['bp_percent_change']:+.1f}%)\")\n",
    "\n",
    "print(f\"\\n  ü´Å Respiration Rate:\")\n",
    "print(f\"     Early: {test_patient['resp_early_mean']:.1f} br/min ‚Üí Late: {test_patient['resp_late_mean']:.1f} br/min\")\n",
    "print(f\"     Change: {test_patient['resp_change']:+.1f} br/min ({test_patient['resp_percent_change']:+.1f}%)\")\n",
    "\n",
    "print(f\"\\n  üíâ Clinical Metrics:\")\n",
    "print(f\"     Shock Index: {test_patient['shock_index']:.2f} ({'‚ö†Ô∏è HIGH' if test_patient['shock_index'] > 0.9 else '‚úì Normal' if test_patient['shock_index'] < 0.7 else '‚ö° Elevated'})\")\n",
    "\n",
    "print(f\"\\nüìù READY FOR LLM EXPLANATION!\")\n",
    "print(f\"Next: We'll send this data to Claude API for clinical interpretation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3085d0-67ee-411e-817e-e3e11f7cf91a",
   "metadata": {},
   "source": [
    "## Step 24: Create Clinical Prompt for LLM Explanation\n",
    "\n",
    "Now we create a structured prompt that will be sent to Claude API to generate clinical explanations.\n",
    "\n",
    "**What is a prompt?**\n",
    "A prompt is the input text we send to the LLM (Large Language Model). It provides context and instructions for what we want the AI to generate.\n",
    "\n",
    "**What this function does:**\n",
    "The `create_clinical_prompt()` function takes:\n",
    "- **Input:** Patient vital signs data, ML prediction, and probability\n",
    "- **Output:** Formatted text prompt ready for Claude API\n",
    "\n",
    "**Prompt Structure:**\n",
    "1. **Role setting:** \"You are a clinical expert analyzing ICU patient vital signs\"\n",
    "2. **Context:** Patient monitoring duration and time period\n",
    "3. **Vital signs data:** \n",
    "   - Heart Rate (early, late, change, average)\n",
    "   - Blood Pressure (early, late, change, average)\n",
    "   - Respiration Rate (early, late, change, average)\n",
    "4. **Clinical metrics:** Shock index with normal reference range\n",
    "5. **ML assessment:** Risk probability and classification\n",
    "6. **Instructions:** Specific request for 3-4 sentence clinical assessment\n",
    "\n",
    "**Why this format works:**\n",
    "- **Structured data:** LLM receives organized, clear information\n",
    "- **Clinical context:** Includes reference ranges (e.g., shock index <0.7)\n",
    "- **Specific instructions:** Tells LLM exactly what to analyze and explain\n",
    "- **Professional tone:** Requests medical-grade explanation\n",
    "\n",
    "**What we're asking Claude to do:**\n",
    "1. Interpret the vital sign patterns\n",
    "2. Assess deterioration risk\n",
    "3. Identify key concerning or reassuring findings\n",
    "4. Provide clinical reasoning\n",
    "\n",
    "**Test output:**\n",
    "This code generates a sample prompt using the test patient and displays it so we can verify:\n",
    "- ‚úÖ All vital signs are included\n",
    "- ‚úÖ Values are formatted correctly  \n",
    "- ‚úÖ Instructions are clear\n",
    "- ‚úÖ Ready to send to Claude API\n",
    "\n",
    "**Next step:**\n",
    "After verifying the prompt looks good, we'll integrate with Claude API to actually generate explanations for all patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fbb5e19e-659f-45c5-917f-d6f75b0a5043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù CLINICAL PROMPT FOR LLM:\n",
      "======================================================================\n",
      "You are a clinical expert analyzing ICU patient vital signs.\n",
      "\n",
      "PATIENT MONITORING DATA (4.8 hour period):\n",
      "\n",
      "HEART RATE:\n",
      "- Early period: 68.1 bpm\n",
      "- Late period: 78.7 bpm\n",
      "- Change: +10.6 bpm (+15.6%)\n",
      "- Average: 73.4 bpm\n",
      "\n",
      "BLOOD PRESSURE (Systolic):\n",
      "- Early period: 152.3 mmHg\n",
      "- Late period: 154.4 mmHg\n",
      "- Change: +2.2 mmHg (+1.4%)\n",
      "- Average: 153.3 mmHg\n",
      "\n",
      "RESPIRATION RATE:\n",
      "- Early period: 22.0 breaths/min\n",
      "- Late period: 19.6 breaths/min\n",
      "- Change: -2.4 breaths/min (-10.7%)\n",
      "- Average: 20.8 breaths/min\n",
      "\n",
      "CLINICAL METRICS:\n",
      "- Shock Index: 0.48 (normal <0.7)\n",
      "\n",
      "ML MODEL ASSESSMENT:\n",
      "- Deterioration Risk: 11.0%\n",
      "- Classification: STABLE\n",
      "\n",
      "Provide a clinical assessment in 3-4 sentences explaining:\n",
      "1. What these vital sign patterns indicate\n",
      "2. Whether this patient shows signs of deterioration\n",
      "3. Key concerning or reassuring findings\n",
      "4. Clinical reasoning for the assessment\n",
      "\n",
      "Be specific, professional, and focus on the most clinically significant changes.\n",
      "\n",
      "======================================================================\n",
      "\n",
      "‚úÖ Prompt ready to send to Claude API!\n",
      "\n",
      "üí° To actually call Claude API, you need:\n",
      "   1. Anthropic API key\n",
      "   2. anthropic Python package\n",
      "\n",
      "For now, this demonstrates what we would send to Claude.\n"
     ]
    }
   ],
   "source": [
    "# Create LLM explanation prompt\n",
    "def create_clinical_prompt(patient_data, prediction, probability):\n",
    "    \"\"\"Generate prompt for Claude to explain the prediction\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"You are a clinical expert analyzing ICU patient vital signs.\n",
    "\n",
    "PATIENT MONITORING DATA ({patient_data['duration_hours']:.1f} hour period):\n",
    "\n",
    "HEART RATE:\n",
    "- Early period: {patient_data['hr_early_mean']:.1f} bpm\n",
    "- Late period: {patient_data['hr_late_mean']:.1f} bpm\n",
    "- Change: {patient_data['hr_change']:+.1f} bpm ({patient_data['hr_percent_change']:+.1f}%)\n",
    "- Average: {patient_data['hr_mean']:.1f} bpm\n",
    "\n",
    "BLOOD PRESSURE (Systolic):\n",
    "- Early period: {patient_data['bp_early_mean']:.1f} mmHg\n",
    "- Late period: {patient_data['bp_late_mean']:.1f} mmHg\n",
    "- Change: {patient_data['bp_change']:+.1f} mmHg ({patient_data['bp_percent_change']:+.1f}%)\n",
    "- Average: {patient_data['bp_systolic_mean']:.1f} mmHg\n",
    "\n",
    "RESPIRATION RATE:\n",
    "- Early period: {patient_data['resp_early_mean']:.1f} breaths/min\n",
    "- Late period: {patient_data['resp_late_mean']:.1f} breaths/min\n",
    "- Change: {patient_data['resp_change']:+.1f} breaths/min ({patient_data['resp_percent_change']:+.1f}%)\n",
    "- Average: {patient_data['resp_rate_mean']:.1f} breaths/min\n",
    "\n",
    "CLINICAL METRICS:\n",
    "- Shock Index: {patient_data['shock_index']:.2f} (normal <0.7)\n",
    "\n",
    "ML MODEL ASSESSMENT:\n",
    "- Deterioration Risk: {probability:.1%}\n",
    "- Classification: {\"DETERIORATING\" if prediction == 1 else \"STABLE\"}\n",
    "\n",
    "Provide a clinical assessment in 3-4 sentences explaining:\n",
    "1. What these vital sign patterns indicate\n",
    "2. Whether this patient shows signs of deterioration\n",
    "3. Key concerning or reassuring findings\n",
    "4. Clinical reasoning for the assessment\n",
    "\n",
    "Be specific, professional, and focus on the most clinically significant changes.\"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Test prompt generation\n",
    "test_prompt = create_clinical_prompt(test_patient, prediction, probability)\n",
    "\n",
    "print(\"üìù CLINICAL PROMPT FOR LLM:\")\n",
    "print(\"=\"*70)\n",
    "print(test_prompt)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\n‚úÖ Prompt ready to send to Claude API!\")\n",
    "print(\"\\nüí° To actually call Claude API, you need:\")\n",
    "print(\"   1. Anthropic API key\")\n",
    "print(\"   2. anthropic Python package\")\n",
    "print(\"\\nFor now, this demonstrates what we would send to Claude.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103a310a-d5ee-4266-99f5-8ba5fa31da19",
   "metadata": {},
   "source": [
    "## Step 25: Test Complete ML + LLM System\n",
    "\n",
    "Now we test the entire end-to-end system: ML prediction ‚Üí LLM explanation.\n",
    "\n",
    "**What this code does:**\n",
    "1. Loads the final labeled dataset\n",
    "2. Selects one test patient\n",
    "3. Gets ML model prediction (stable/deteriorating + probability)\n",
    "4. Displays vital signs summary\n",
    "5. Creates clinical prompt with all patient data\n",
    "6. Calls Claude API to generate explanation\n",
    "7. Displays the LLM's clinical interpretation\n",
    "\n",
    "**The Complete Pipeline:**\n",
    "```\n",
    "Patient Data ‚Üí ML Model ‚Üí Prediction + Probability\n",
    "                              ‚Üì\n",
    "                    Format Clinical Prompt\n",
    "                              ‚Üì\n",
    "                         Claude API\n",
    "                              ‚Üì\n",
    "                    Clinical Explanation\n",
    "```\n",
    "\n",
    "**What you should see:**\n",
    "1. **Patient identification:** ID and segment number\n",
    "2. **ML prediction:** Classification (stable/deteriorating) and risk %\n",
    "3. **Vital signs summary:** HR, BP, RR changes with arrows showing trends\n",
    "4. **Claude's explanation:** 3-4 sentences of clinical reasoning including:\n",
    "   - Interpretation of vital sign patterns\n",
    "   - Assessment of deterioration risk\n",
    "   - Key concerning or reassuring findings\n",
    "   - Clinical reasoning and recommendations\n",
    "\n",
    "**Why this test is important:**\n",
    "- Verifies ML model works correctly\n",
    "- Tests Claude API connection and authentication\n",
    "- Validates prompt format produces good explanations\n",
    "- Confirms entire system integrates properly\n",
    "- Shows what the final demo will look like\n",
    "\n",
    "**Expected Claude response:**\n",
    "For a **stable** patient, Claude might say:\n",
    "> \"This patient demonstrates stable hemodynamics with reassuring vital sign trends. The heart rate increase is moderate and accompanied by stable blood pressure, indicating adequate cardiovascular compensation. The shock index remains well within normal limits, and the absence of concurrent hypotension rules out compensatory shock.\"\n",
    "\n",
    "For a **deteriorating** patient, Claude might say:\n",
    "> \"‚ö†Ô∏è This patient exhibits concerning signs of early compensatory shock requiring immediate attention. The combination of rising heart rate and falling blood pressure represents classic hemodynamic decompensation. The elevated shock index exceeds critical thresholds, indicating significant hemodynamic instability that typically precedes cardiovascular collapse by 4-6 hours.\"\n",
    "\n",
    "**Troubleshooting:**\n",
    "- If you see \"Error calling GPT/Claude\": Check API key and credits\n",
    "- If explanation is too short/generic: Adjust prompt instructions\n",
    "- If formatting is off: Check that all patient features exist\n",
    "\n",
    "**Success indicator:** ‚úÖ You should see a detailed, clinically accurate explanation that makes sense given the vital signs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f66f44f-96e4-4370-8807-e4218e991cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\river\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\river\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ TESTING COMPLETE SYSTEM: ML + LLM\n",
      "======================================================================\n",
      "\n",
      "üë§ Patient: 30/3000393\n",
      "üìã Segment: 3000393_0005\n",
      "‚è±Ô∏è  Duration: 4.8 hours\n",
      "\n",
      "ü§ñ ML MODEL PREDICTION:\n",
      "   Classification: ‚úÖ STABLE\n",
      "   Risk Probability: 11.0%\n",
      "\n",
      "üìä VITAL SIGNS SUMMARY:\n",
      "   üíì HR:  68.1 ‚Üí 78.7 bpm (+10.6)\n",
      "   ü©∫ BP:  152.3 ‚Üí 154.4 mmHg (+2.2)\n",
      "   ü´Å RR:  22.0 ‚Üí 19.6 br/min (-2.4)\n",
      "   üíâ Shock Index: 0.48\n",
      "\n",
      "ü§ñ Generating GPT explanation...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_llm_explanation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 39\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mü§ñ Generating GPT explanation...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     38\u001b[0m prompt \u001b[38;5;241m=\u001b[39m create_clinical_prompt(test_patient, prediction, probability)\n\u001b[1;32m---> 39\u001b[0m explanation \u001b[38;5;241m=\u001b[39m get_llm_explanation(prompt)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müí¨ GPT CLINICAL EXPLANATION:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m70\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_llm_explanation' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 4: Test LLM explanation on ONE patient\n",
    "\n",
    "# Load your final dataset\n",
    "final_data = pd.read_csv('FINAL_COMPLETE_DATASET.csv')\n",
    "\n",
    "# Select first patient for testing\n",
    "test_idx = 0\n",
    "test_patient = final_data.iloc[test_idx]\n",
    "\n",
    "# Prepare features for ML model\n",
    "feature_columns = [col for col in final_data.columns \n",
    "                   if col not in ['patient_id', 'segment', 'deterioration']]\n",
    "test_features_input = test_patient[feature_columns].values.reshape(1, -1)\n",
    "\n",
    "# Get ML prediction\n",
    "prediction = model.predict(test_features_input)[0]\n",
    "probability = model.predict_proba(test_features_input)[0, 1]\n",
    "\n",
    "# Display test case\n",
    "print(\"üî¨ TESTING COMPLETE SYSTEM: ML + LLM\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüë§ Patient: {test_patient['patient_id']}\")\n",
    "print(f\"üìã Segment: {test_patient['segment']}\")\n",
    "print(f\"‚è±Ô∏è  Duration: {test_patient['duration_hours']:.1f} hours\")\n",
    "\n",
    "print(f\"\\nü§ñ ML MODEL PREDICTION:\")\n",
    "print(f\"   Classification: {'‚ö†Ô∏è DETERIORATING' if prediction == 1 else '‚úÖ STABLE'}\")\n",
    "print(f\"   Risk Probability: {probability:.1%}\")\n",
    "\n",
    "print(f\"\\nüìä VITAL SIGNS SUMMARY:\")\n",
    "print(f\"   üíì HR:  {test_patient['hr_early_mean']:.1f} ‚Üí {test_patient['hr_late_mean']:.1f} bpm ({test_patient['hr_change']:+.1f})\")\n",
    "print(f\"   ü©∫ BP:  {test_patient['bp_early_mean']:.1f} ‚Üí {test_patient['bp_late_mean']:.1f} mmHg ({test_patient['bp_change']:+.1f})\")\n",
    "print(f\"   ü´Å RR:  {test_patient['resp_early_mean']:.1f} ‚Üí {test_patient['resp_late_mean']:.1f} br/min ({test_patient['resp_change']:+.1f})\")\n",
    "print(f\"   üíâ Shock Index: {test_patient['shock_index']:.2f}\")\n",
    "\n",
    "# Generate LLM explanation\n",
    "print(f\"\\nü§ñ Generating GPT explanation...\")\n",
    "prompt = create_clinical_prompt(test_patient, prediction, probability)\n",
    "explanation = get_llm_explanation(prompt)\n",
    "\n",
    "print(f\"\\nüí¨ GPT CLINICAL EXPLANATION:\")\n",
    "print(\"=\"*70)\n",
    "print(explanation)\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n‚úÖ SYSTEM WORKS! ML prediction + LLM explanation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6b016f-f745-43b0-853e-d04499ac9d4b",
   "metadata": {},
   "source": [
    "# LLM Integration - Claude API\n",
    "\n",
    "Now we integrate Claude AI to generate clinical explanations for our ML predictions.\n",
    "\n",
    "## Step 26: Install Anthropic Package\n",
    "\n",
    "Install the official Anthropic Python SDK to communicate with Claude API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822be60d-6ad9-474c-abe1-2a5f5aa54d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Anthropic package\n",
    "!pip install anthropic\n",
    "\n",
    "print(\"‚úÖ Anthropic package installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7ce5eb-ad83-4917-bb77-8f7d1756f4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Claude API\n",
    "import anthropic\n",
    "\n",
    "# REPLACE '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' with your actual API key!\n",
    "client = anthropic.Anthropic(\n",
    "    api_key=\"sk-ant-a...\"  # ‚Üê PASTE YOUR KEY HERE\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Claude API client initialized!\")\n",
    "print(\"üîë API key loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d2d666-e164-477a-bfac-075435680e38",
   "metadata": {},
   "source": [
    "## Step 28: Create LLM Explanation Function\n",
    "\n",
    "Define the function that sends clinical prompts to Claude and receives explanations.\n",
    "\n",
    "**What this function does:**\n",
    "- Takes a clinical prompt as input\n",
    "- Sends it to Claude API using the Sonnet 4 model\n",
    "- Returns Claude's clinical explanation\n",
    "- Handles errors gracefully if API call fails\n",
    "\n",
    "**Model:** `claude-sonnet-4-20250514` (best for medical reasoning)\n",
    "\n",
    "**Cost:** ~$0.003 per explanation (very cheap!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b040bd-9025-4c2b-a8bc-14ce4ade2dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LLM explanation function\n",
    "def get_llm_explanation(prompt):\n",
    "    \"\"\"Call Claude API for clinical explanation\"\"\"\n",
    "    \n",
    "    try:\n",
    "        message = client.messages.create(\n",
    "            model=\"claude-sonnet-4-20250514\",\n",
    "            max_tokens=500,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        return message.content[0].text\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Error calling Claude: {str(e)}\"\n",
    "\n",
    "print(\"‚úÖ LLM explanation function ready!\")\n",
    "print(\"ü§ñ Claude Sonnet 4 configured for clinical reasoning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342b7b94-6ef4-46a8-bcc4-4a5a17b26689",
   "metadata": {},
   "source": [
    "## Step 29: Test Complete ML + LLM System on One Patient\n",
    "\n",
    "Test the entire end-to-end system: ML prediction ‚Üí Clinical prompt ‚Üí Claude explanation.\n",
    "\n",
    "**What this code does:**\n",
    "1. Loads the final labeled dataset\n",
    "2. Selects one test patient\n",
    "3. Gets ML model prediction (stable/deteriorating + probability)\n",
    "4. Displays vital signs summary\n",
    "5. Creates clinical prompt with all patient data\n",
    "6. Calls Claude API to generate explanation\n",
    "7. Displays Claude's clinical interpretation\n",
    "\n",
    "**Expected result:** \n",
    "You should see ML prediction + detailed vital signs + Claude's 3-4 sentence clinical explanation.\n",
    "\n",
    "**Cost:** ~$0.003 (less than 1 cent) for this one test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d031ed9-a43e-4332-be57-4339a68cc5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test LLM explanation on ONE patient\n",
    "\n",
    "# Load your final dataset\n",
    "final_data = pd.read_csv('FINAL_COMPLETE_DATASET.csv')\n",
    "\n",
    "# Select first patient for testing\n",
    "test_idx = 0\n",
    "test_patient = final_data.iloc[test_idx]\n",
    "\n",
    "# Prepare features for ML model\n",
    "feature_columns = [col for col in final_data.columns \n",
    "                   if col not in ['patient_id', 'segment', 'deterioration']]\n",
    "test_features_input = test_patient[feature_columns].values.reshape(1, -1)\n",
    "\n",
    "# Get ML prediction\n",
    "prediction = model.predict(test_features_input)[0]\n",
    "probability = model.predict_proba(test_features_input)[0, 1]\n",
    "\n",
    "# Display test case\n",
    "print(\"üî¨ TESTING COMPLETE SYSTEM: ML + LLM\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüë§ Patient: {test_patient['patient_id']}\")\n",
    "print(f\"üìã Segment: {test_patient['segment']}\")\n",
    "print(f\"‚è±Ô∏è  Duration: {test_patient['duration_hours']:.1f} hours\")\n",
    "\n",
    "print(f\"\\nü§ñ ML MODEL PREDICTION:\")\n",
    "print(f\"   Classification: {'‚ö†Ô∏è DETERIORATING' if prediction == 1 else '‚úÖ STABLE'}\")\n",
    "print(f\"   Risk Probability: {probability:.1%}\")\n",
    "\n",
    "print(f\"\\nüìä VITAL SIGNS SUMMARY:\")\n",
    "print(f\"   üíì HR:  {test_patient['hr_early_mean']:.1f} ‚Üí {test_patient['hr_late_mean']:.1f} bpm ({test_patient['hr_change']:+.1f})\")\n",
    "print(f\"   ü©∫ BP:  {test_patient['bp_early_mean']:.1f} ‚Üí {test_patient['bp_late_mean']:.1f} mmHg ({test_patient['bp_change']:+.1f})\")\n",
    "print(f\"   ü´Å RR:  {test_patient['resp_early_mean']:.1f} ‚Üí {test_patient['resp_late_mean']:.1f} br/min ({test_patient['resp_change']:+.1f})\")\n",
    "print(f\"   üíâ Shock Index: {test_patient['shock_index']:.2f}\")\n",
    "\n",
    "# Generate Claude explanation\n",
    "print(f\"\\nü§ñ Generating Claude explanation...\")\n",
    "prompt = create_clinical_prompt(test_patient, prediction, probability)\n",
    "explanation = get_llm_explanation(prompt)\n",
    "\n",
    "print(f\"\\nüí¨ CLAUDE CLINICAL EXPLANATION:\")\n",
    "print(\"=\"*70)\n",
    "print(explanation)\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n‚úÖ SYSTEM WORKS! ML prediction + LLM explanation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1639e719-bf5c-497d-bc37-eb4129bcf2b6",
   "metadata": {},
   "source": [
    "## Step 30: Generate Claude Explanations for All 91 Patients\n",
    "\n",
    "Now that we've verified the system works on one patient, let's generate clinical explanations for all segments in our dataset.\n",
    "\n",
    "**What this code does:**\n",
    "1. Loops through all 91 segments\n",
    "2. For each segment:\n",
    "   - Gets ML prediction\n",
    "   - Creates clinical prompt\n",
    "   - Calls Claude API for explanation\n",
    "   - Saves results\n",
    "3. Shows progress every 10 patients\n",
    "4. Saves complete results to CSV\n",
    "\n",
    "**Processing time:** ~3-5 minutes (91 API calls)\n",
    "**Cost:** ~$0.27 (27 cents total)\n",
    "\n",
    "**Output file:** `ML_CLAUDE_EXPLANATIONS.csv`\n",
    "- Contains: Patient ID, ML prediction, probability, Claude explanation\n",
    "- Ready for analysis and presentation\n",
    "- Can be used to compare stable vs deteriorating patient explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af551d3f-9cc1-42ac-bea6-1331e9a85ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Claude explanations for ALL 91 patients\n",
    "\n",
    "print(\"üöÄ Generating Claude explanations for all 91 patients...\")\n",
    "print(\"üí∞ Cost: ~$0.27 (27 cents total)\")\n",
    "print(\"‚è±Ô∏è  Time: ~3-5 minutes\\n\")\n",
    "\n",
    "all_explanations = []\n",
    "\n",
    "for idx, row in final_data.iterrows():\n",
    "    try:\n",
    "        # Prepare features\n",
    "        features_input = row[feature_columns].values.reshape(1, -1)\n",
    "        \n",
    "        # Get ML prediction\n",
    "        pred = model.predict(features_input)[0]\n",
    "        prob = model.predict_proba(features_input)[0, 1]\n",
    "        \n",
    "        # Generate prompt\n",
    "        prompt = create_clinical_prompt(row, pred, prob)\n",
    "        \n",
    "        # Get Claude explanation\n",
    "        explanation = get_llm_explanation(prompt)\n",
    "        \n",
    "        # Store result\n",
    "        all_explanations.append({\n",
    "            'patient_id': row['patient_id'],\n",
    "            'segment': row['segment'],\n",
    "            'ml_prediction': 'DETERIORATING' if pred == 1 else 'STABLE',\n",
    "            'ml_probability': prob,\n",
    "            'claude_explanation': explanation\n",
    "        })\n",
    "        \n",
    "        if (idx + 1) % 10 == 0:\n",
    "            print(f\"‚úÖ Processed {idx + 1}/91 patients...\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error on patient {idx}: {str(e)}\")\n",
    "\n",
    "print(f\"\\nüéâ COMPLETE! Generated {len(all_explanations)} explanations\")\n",
    "\n",
    "# Save to CSV\n",
    "explanations_df = pd.DataFrame(all_explanations)\n",
    "explanations_df.to_csv('ML_CLAUDE_EXPLANATIONS.csv', index=False)\n",
    "\n",
    "print(f\"üíæ Saved to: ML_CLAUDE_EXPLANATIONS.csv\")\n",
    "print(f\"üìä Dataset: {len(explanations_df)} patients √ó {len(explanations_df.columns)} columns\")\n",
    "\n",
    "# Show summary statistics\n",
    "print(f\"\\nüìà Summary:\")\n",
    "print(f\"   Stable predictions: {(explanations_df['ml_prediction']=='STABLE').sum()}\")\n",
    "print(f\"   Deteriorating predictions: {(explanations_df['ml_prediction']=='DETERIORATING').sum()}\")\n",
    "\n",
    "# Show example explanations\n",
    "print(f\"\\nüìã Example explanations:\\n\")\n",
    "for i in range(min(3, len(explanations_df))):\n",
    "    print(f\"Patient {i+1}: {explanations_df.iloc[i]['patient_id']}\")\n",
    "    print(f\"Prediction: {explanations_df.iloc[i]['ml_prediction']} ({explanations_df.iloc[i]['ml_probability']:.1%})\")\n",
    "    print(f\"Claude: {explanations_df.iloc[i]['claude_explanation'][:200]}...\")\n",
    "    print(\"-\"*70)\n",
    "\n",
    "print(\"\\n‚úÖ ALL DONE! Your complete ML + LLM system is ready for presentation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181f9529-1f7c-4390-93f4-14a1bc190588",
   "metadata": {},
   "source": [
    "# PART 2: Corrected Methodology - Patient-Level Splitting\n",
    "\n",
    "## Identifying and Fixing Data Leakage\n",
    "\n",
    "**Problem discovered:** Our initial approach treated segments from the same patient as independent examples. This violates the independence assumption in machine learning and can lead to optimistic performance estimates.\n",
    "\n",
    "**Our current code does this:** \n",
    "train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "**This randomly splits SEGMENTS, so we might get:**\n",
    "TRAINING SET:\n",
    "- 3000003_0001 ‚úì\n",
    "- 3000003_0002 ‚úì  \n",
    "- 3000031_0001 ‚úì\n",
    "- 3000393_0005 ‚úì\n",
    "\n",
    "**TESTING SET:**\n",
    "- 3000003_0005 ‚Üê PROBLEM! Same patient as training!\n",
    "- 3000031_0004 ‚Üê PROBLEM! Same patient as training!\n",
    "\n",
    "**Solution:** Split data by PATIENT first, ensuring no patient appears in both training and testing sets.\n",
    "\n",
    "**Expected impact:** More honest accuracy estimate (likely 5-15% lower, but methodologically correct)\n",
    "\n",
    "---\n",
    "\n",
    "## Step 31: Analyze Patient Distribution\n",
    "\n",
    "Let's first understand how many unique patients we have and how segments are distributed across patients.\n",
    "\n",
    "**This analysis will show us:**\n",
    "- How many segments each patient has\n",
    "- Confirm no patient should appear in both train and test sets\n",
    "- Implement the correct patient-level train/test split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d91f3f0-ad64-4c1f-bd0e-a78ad1ff9209",
   "metadata": {},
   "source": [
    "### Load and Analyze patient distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de66ef33-2c65-416e-89a7-0b1865fc0932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Overview:\n",
      "Total segments: 91\n",
      "Unique patients: 27\n",
      "Average segments per patient: 3.4\n",
      "\n",
      "Segments per patient:\n",
      "Maximum: 12\n",
      "Minimum: 1\n",
      "Median: 2.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "# Load your complete dataset\n",
    "df = pd.read_csv('FINAL_COMPLETE_DATASET.csv')\n",
    "\n",
    "print(\"Dataset Overview:\")\n",
    "print(f\"Total segments: {len(df)}\")\n",
    "print(f\"Unique patients: {df['patient_id'].nunique()}\")\n",
    "print(f\"Average segments per patient: {len(df) / df['patient_id'].nunique():.1f}\")\n",
    "\n",
    "# Show distribution of segments per patient\n",
    "segments_per_patient = df.groupby('patient_id').size().sort_values(ascending=False)\n",
    "print(f\"\\nSegments per patient:\")\n",
    "print(f\"Maximum: {segments_per_patient.max()}\")\n",
    "print(f\"Minimum: {segments_per_patient.min()}\")\n",
    "print(f\"Median: {segments_per_patient.median():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf5587e-04d5-4e81-9fc7-41fd8f5e786d",
   "metadata": {},
   "source": [
    "### Implement Patient-Level Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d37e6208-87e5-4921-b56c-41e4811e4d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique patients: 27\n",
      "\n",
      "Patient-Level Split:\n",
      "Training patients: 21\n",
      "Testing patients: 6\n",
      "\n",
      "Segment Distribution:\n",
      "Training segments: 69 (from 21 patients)\n",
      "Testing segments: 22 (from 6 patients)\n",
      "\n",
      "Verification - Patient overlap: 0 (should be 0)\n"
     ]
    }
   ],
   "source": [
    "# Get unique patient IDs\n",
    "unique_patients = df['patient_id'].unique()\n",
    "print(f\"Total unique patients: {len(unique_patients)}\")\n",
    "\n",
    "# Split PATIENTS (not segments) into train/test\n",
    "train_patients, test_patients = train_test_split(\n",
    "    unique_patients, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nPatient-Level Split:\")\n",
    "print(f\"Training patients: {len(train_patients)}\")\n",
    "print(f\"Testing patients: {len(test_patients)}\")\n",
    "\n",
    "# Create train/test sets based on patient assignment\n",
    "train_data = df[df['patient_id'].isin(train_patients)]\n",
    "test_data = df[df['patient_id'].isin(test_patients)]\n",
    "\n",
    "print(f\"\\nSegment Distribution:\")\n",
    "print(f\"Training segments: {len(train_data)} (from {len(train_patients)} patients)\")\n",
    "print(f\"Testing segments: {len(test_data)} (from {len(test_patients)} patients)\")\n",
    "\n",
    "# Verify no patient overlap\n",
    "train_patient_check = set(train_data['patient_id'].unique())\n",
    "test_patient_check = set(test_data['patient_id'].unique())\n",
    "overlap = train_patient_check.intersection(test_patient_check)\n",
    "print(f\"\\nVerification - Patient overlap: {len(overlap)} (should be 0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37da53d8-5103-4c7b-a332-d9c73a36cdb6",
   "metadata": {},
   "source": [
    "### Train Corrected Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5e8226c-7354-47b0-97f1-dd136c52a74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest with patient-level split...\n",
      "\n",
      "CORRECTED Model Performance:\n",
      "==================================================\n",
      "Training Accuracy: 100.0%\n",
      "Testing Accuracy: 86.4%\n",
      "\n",
      "Test Set Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       STABLE       0.84      1.00      0.91        16\n",
      "DETERIORATING       1.00      0.50      0.67         6\n",
      "\n",
      "     accuracy                           0.86        22\n",
      "    macro avg       0.92      0.75      0.79        22\n",
      " weighted avg       0.89      0.86      0.85        22\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare features and labels\n",
    "feature_columns = [col for col in df.columns \n",
    "                  if col not in ['patient_id', 'segment', 'deterioration']]\n",
    "\n",
    "X_train = train_data[feature_columns].values\n",
    "y_train = train_data['deterioration'].values\n",
    "X_test = test_data[feature_columns].values\n",
    "y_test = test_data['deterioration'].values\n",
    "\n",
    "# Train new model with correct split\n",
    "print(\"Training Random Forest with patient-level split...\")\n",
    "model_fixed = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    random_state=42,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "model_fixed.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate performance\n",
    "train_pred = model_fixed.predict(X_train)\n",
    "test_pred = model_fixed.predict(X_test)\n",
    "\n",
    "print(\"\\nCORRECTED Model Performance:\")\n",
    "print(\"=\"*50)\n",
    "print(\"Training Accuracy: {:.1f}%\".format(accuracy_score(y_train, train_pred) * 100))\n",
    "print(\"Testing Accuracy: {:.1f}%\".format(accuracy_score(y_test, test_pred) * 100))\n",
    "\n",
    "print(\"\\nTest Set Classification Report:\")\n",
    "print(classification_report(y_test, test_pred, \n",
    "                          target_names=['STABLE', 'DETERIORATING']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2e3df1-9d5c-4602-a56b-d898ec6eff61",
   "metadata": {},
   "source": [
    "### Compare Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e5a3a0e-7327-42d8-b977-bec780917af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPARISON WITH INCORRECT METHOD:\n",
      "==================================================\n",
      "Old method (data leakage): 89.5% accuracy\n",
      "Correct method (patient-split): 86.4% accuracy\n",
      "Difference: 3.1%\n",
      "\n",
      "The correct method gives a more honest estimate of real-world performance!\n"
     ]
    }
   ],
   "source": [
    "# Wrong way (for comparison)\n",
    "X_train_wrong, X_test_wrong, y_train_wrong, y_test_wrong = train_test_split(\n",
    "    df[feature_columns].values, \n",
    "    df['deterioration'].values,\n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model_wrong = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model_wrong.fit(X_train_wrong, y_train_wrong)\n",
    "\n",
    "wrong_test_acc = accuracy_score(y_test_wrong, model_wrong.predict(X_test_wrong))\n",
    "correct_test_acc = accuracy_score(y_test, test_pred)\n",
    "\n",
    "print(\"COMPARISON WITH INCORRECT METHOD:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Old method (data leakage): {wrong_test_acc:.1%} accuracy\")\n",
    "print(f\"Correct method (patient-split): {correct_test_acc:.1%} accuracy\")\n",
    "print(f\"Difference: {(wrong_test_acc - correct_test_acc):.1%}\")\n",
    "print(\"\\nThe correct method gives a more honest estimate of real-world performance!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8d517cb-42ab-4f98-9609-b58add4f3243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved corrected model to: final_model_patient_split_CORRECT.pkl\n",
      "Saved split information to: patient_split_info.json\n"
     ]
    }
   ],
   "source": [
    "# Save the correctly trained model\n",
    "joblib.dump(model_fixed, 'final_model_patient_split_CORRECT.pkl')\n",
    "print(\"Saved corrected model to: final_model_patient_split_CORRECT.pkl\")\n",
    "\n",
    "# Save the train/test split info for documentation\n",
    "split_info = {\n",
    "    'train_patients': train_patients.tolist(),\n",
    "    'test_patients': test_patients.tolist(),\n",
    "    'train_segments': len(train_data),\n",
    "    'test_segments': len(test_data),\n",
    "    'test_accuracy': correct_test_acc\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('patient_split_info.json', 'w') as f:\n",
    "    json.dump(split_info, f, indent=2)\n",
    "print(\"Saved split information to: patient_split_info.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c836457-0d31-457b-80cd-9722d994b7e3",
   "metadata": {},
   "source": [
    "## Finally! The model is corrected. Let's Generate Claude Explanations using the corrected model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "daf69e74-38fc-43cc-8360-9d99fa2c6114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in your dataframe:\n",
      "['patient_id', 'segment', 'duration_hours', 'hr_mean', 'hr_std', 'hr_min', 'hr_max', 'hr_early_mean', 'hr_late_mean', 'hr_change', 'hr_percent_change', 'bp_systolic_mean', 'bp_systolic_std', 'bp_systolic_min', 'bp_systolic_max', 'bp_early_mean', 'bp_late_mean', 'bp_change', 'bp_percent_change', 'resp_rate_mean', 'resp_rate_std', 'resp_rate_min', 'resp_rate_max', 'resp_early_mean', 'resp_late_mean', 'resp_change', 'resp_percent_change', 'pleth_quality', 'pleth_mean', 'pleth_std', 'shock_index', 'deterioration']\n",
      "\n",
      "\n",
      "Sample data from first row:\n",
      "patient_id: 30/3000393\n",
      "segment: 3000393_0005\n",
      "duration_hours: 4.813055555555556\n",
      "hr_mean: 73.35714780031789\n",
      "hr_std: 8.054185650837764\n",
      "hr_min: 51.02040816326531\n",
      "hr_max: 119.04761904761904\n",
      "hr_early_mean: 68.05995996401344\n",
      "hr_late_mean: 78.65433563662232\n",
      "hr_change: 10.594375672608876\n",
      "hr_percent_change: 15.566238472973874\n",
      "bp_systolic_mean: 153.3414553051391\n",
      "bp_systolic_std: 16.460775730936902\n",
      "bp_systolic_min: 51.48168590310307\n",
      "bp_systolic_max: 185.0214669838277\n",
      "bp_early_mean: 152.2562462352903\n",
      "bp_late_mean: 154.4266643749879\n",
      "bp_change: 2.1704181396976026\n",
      "bp_percent_change: 1.4255035135593264\n",
      "resp_rate_mean: 20.7767812774872\n",
      "resp_rate_std: 2.0459927148656774\n",
      "resp_rate_min: 4.288164665523156\n",
      "resp_rate_max: 30.0\n",
      "resp_early_mean: 21.9521165980264\n",
      "resp_late_mean: 19.60184316388296\n",
      "resp_change: -2.350273434143446\n",
      "resp_percent_change: -10.706363660416905\n",
      "pleth_quality: 0.3333768171805614\n",
      "pleth_mean: 1.829289474312209\n",
      "pleth_std: 0.4687029629241023\n",
      "shock_index: 0.4783908412395208\n",
      "deterioration: 0\n"
     ]
    }
   ],
   "source": [
    "# Let's see what columns you actually have in your dataset\n",
    "print(\"Columns in your dataframe:\")\n",
    "print(df.columns.tolist())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Show a sample row to understand the data structure\n",
    "print(\"Sample data from first row:\")\n",
    "sample = df.iloc[0]\n",
    "for col in df.columns:\n",
    "    print(f\"{col}: {sample[col]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0284c4c5-a389-4946-8178-68c2e0da2e95",
   "metadata": {},
   "source": [
    "# Step 32: AI EXPLAINABILITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "486acd71-7427-4cf6-8b6f-0bad48cee8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "import os\n",
    "\n",
    "# Initialize Claude client\n",
    "client = anthropic.Anthropic(api_key=os.environ.get('ANTHROPIC_API_KEY'))\n",
    "\n",
    "def create_clinical_prompt(patient_data, ml_prediction, ml_probability):\n",
    "    \"\"\"\n",
    "    Create a clinical prompt for Claude to explain the ML prediction\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"You are an ICU physician reviewing patient vital signs and ML model predictions.\n",
    "\n",
    "Patient Data:\n",
    "- Segment ID: {patient_data['segment']}\n",
    "- Duration: {patient_data['duration_hours']:.1f} hours\n",
    "- ML Prediction: {'DETERIORATING' if ml_prediction == 1 else 'STABLE'}\n",
    "- Risk Probability: {ml_probability:.1%}\n",
    "\n",
    "Vital Sign Changes (Early ‚Üí Late Period):\n",
    "- Heart Rate: {patient_data['hr_early_mean']:.1f} ‚Üí {patient_data['hr_late_mean']:.1f} bpm (Change: {patient_data['hr_percent_change']:.1f}%)\n",
    "- Systolic BP: {patient_data['bp_early_mean']:.1f} ‚Üí {patient_data['bp_late_mean']:.1f} mmHg (Change: {patient_data['bp_percent_change']:.1f}%)\n",
    "- Respiratory Rate: {patient_data['resp_early_mean']:.1f} ‚Üí {patient_data['resp_late_mean']:.1f} br/min (Change: {patient_data['resp_percent_change']:.1f}%)\n",
    "- Shock Index: {patient_data['shock_index']:.2f} (Normal < 0.7)\n",
    "\n",
    "Provide a brief (3-4 sentence) clinical assessment explaining:\n",
    "1. Whether these vital sign trends support the ML prediction\n",
    "2. The most concerning or reassuring findings\n",
    "3. Clinical significance of the patterns observed\n",
    "\n",
    "Be specific about the physiological implications.\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "def get_llm_explanation(prompt):\n",
    "    \"\"\"\n",
    "    Get explanation from Claude API\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.messages.create(\n",
    "            model=\"claude-3-sonnet-20240229\",\n",
    "            max_tokens=300,\n",
    "            temperature=0.3,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        return response.content[0].text\n",
    "    except Exception as e:\n",
    "        return f\"Error getting explanation: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82b53f35-87fb-4eb0-9e2f-8f1c952b9a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Generating Claude explanations with CORRECTED model predictions...\n",
      "This will use the patient-split model for honest predictions\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\river\\AppData\\Local\\Temp\\ipykernel_56812\\94368852.py:39: DeprecationWarning: The model 'claude-3-sonnet-20240229' is deprecated and will reach end-of-life on July 21st, 2025.\n",
      "Please migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.\n",
      "  response = client.messages.create(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processed 10/91 patients...\n",
      "‚úÖ Processed 20/91 patients...\n",
      "‚úÖ Processed 30/91 patients...\n",
      "‚úÖ Processed 40/91 patients...\n",
      "‚úÖ Processed 50/91 patients...\n",
      "‚úÖ Processed 60/91 patients...\n",
      "‚úÖ Processed 70/91 patients...\n",
      "‚úÖ Processed 80/91 patients...\n",
      "‚úÖ Processed 90/91 patients...\n",
      "\n",
      "üíæ Saved to: ML_CLAUDE_EXPLANATIONS_CORRECTED.csv\n",
      "\n",
      "üìã Test Patient: 30/3000393\n",
      "Prediction: STABLE (23.0%)\n",
      "Claude: Error getting explanation: \"Could not resolve authentication method. Expected either api_key or auth_token to be set. Or for one of the `X-Api-Key` or `Authorization` headers to be explicitly omitted\"...\n",
      "\n",
      "üìã Test Patient: 30/3000393\n",
      "Prediction: DETERIORATING (61.0%)\n",
      "Claude: Error getting explanation: \"Could not resolve authentication method. Expected either api_key or auth_token to be set. Or for one of the `X-Api-Key` or `Authorization` headers to be explicitly omitted\"...\n"
     ]
    }
   ],
   "source": [
    "# Generate Claude explanations for CORRECTED predictions\n",
    "import time\n",
    "\n",
    "print(\"üöÄ Generating Claude explanations with CORRECTED model predictions...\")\n",
    "print(\"This will use the patient-split model for honest predictions\\n\")\n",
    "\n",
    "# Get predictions from CORRECTED model for ALL data\n",
    "all_predictions = model_fixed.predict(df[feature_columns].values)\n",
    "all_probabilities = model_fixed.predict_proba(df[feature_columns].values)[:, 1]\n",
    "\n",
    "# Store results\n",
    "corrected_explanations = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    try:\n",
    "        # Use the CORRECTED model's predictions\n",
    "        pred = all_predictions[idx]\n",
    "        prob = all_probabilities[idx]\n",
    "        \n",
    "        # Generate prompt with corrected predictions\n",
    "        prompt = create_clinical_prompt(row, pred, prob)\n",
    "        \n",
    "        # Get Claude explanation\n",
    "        explanation = get_llm_explanation(prompt)\n",
    "        \n",
    "        corrected_explanations.append({\n",
    "            'patient_id': row['patient_id'],\n",
    "            'segment': row['segment'],\n",
    "            'ml_prediction': 'DETERIORATING' if pred == 1 else 'STABLE',\n",
    "            'ml_probability': prob,\n",
    "            'claude_explanation': explanation,\n",
    "            'is_test_set': row['patient_id'] in test_patients  # Mark if it was in test set\n",
    "        })\n",
    "        \n",
    "        if (idx + 1) % 10 == 0:\n",
    "            print(f\"‚úÖ Processed {idx + 1}/91 patients...\")\n",
    "        \n",
    "        # Small delay to avoid rate limiting\n",
    "        time.sleep(0.5)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error on patient {idx}: {str(e)}\")\n",
    "\n",
    "# Save corrected explanations\n",
    "corrected_df = pd.DataFrame(corrected_explanations)\n",
    "corrected_df.to_csv('ML_CLAUDE_EXPLANATIONS_CORRECTED.csv', index=False)\n",
    "print(f\"\\nüíæ Saved to: ML_CLAUDE_EXPLANATIONS_CORRECTED.csv\")\n",
    "\n",
    "# Show example from test set\n",
    "test_examples = corrected_df[corrected_df['is_test_set'] == True].head(2)\n",
    "for _, ex in test_examples.iterrows():\n",
    "    print(f\"\\nüìã Test Patient: {ex['patient_id']}\")\n",
    "    print(f\"Prediction: {ex['ml_prediction']} ({ex['ml_probability']:.1%})\")\n",
    "    print(f\"Claude: {ex['claude_explanation'][:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f85475-ac2b-4990-81cb-19cf5f4acb2f",
   "metadata": {},
   "source": [
    "### API Key not working ... let's try to connect API key again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fdc74c33-4bcf-4222-8431-013f6547b192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Claude API connection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\river\\AppData\\Local\\Temp\\ipykernel_56812\\451353739.py:42: DeprecationWarning: The model 'claude-3-5-sonnet-20241022' is deprecated and will reach end-of-life on October 22, 2025.\n",
      "Please migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.\n",
      "  response = client.messages.create(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test response: Error: Error code: 404 - {'type': 'error', 'error': {'type': 'not_found_error', 'message': 'model: claude-3-5-sonnet-20241022'}, 'request_id': 'req_011CUr3X3CAPwTaz6YpQgce1'}\n"
     ]
    }
   ],
   "source": [
    "import anthropic\n",
    "import os\n",
    "\n",
    "# Set your API key directly (replace with your actual key)\n",
    "api_key = \"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\"  # PUT YOUR ACTUAL KEY HERE\n",
    "\n",
    "# Initialize Claude client with the key directly\n",
    "client = anthropic.Anthropic(api_key=api_key)\n",
    "\n",
    "def create_clinical_prompt(patient_data, ml_prediction, ml_probability):\n",
    "    \"\"\"\n",
    "    Create a clinical prompt for Claude to explain the ML prediction\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"You are an ICU physician reviewing patient vital signs and ML model predictions.\n",
    "\n",
    "Patient Data:\n",
    "- Segment ID: {patient_data['segment']}\n",
    "- Duration: {patient_data['duration_hours']:.1f} hours\n",
    "- ML Prediction: {'DETERIORATING' if ml_prediction == 1 else 'STABLE'}\n",
    "- Risk Probability: {ml_probability:.1%}\n",
    "\n",
    "Vital Sign Changes (Early ‚Üí Late Period):\n",
    "- Heart Rate: {patient_data['hr_early_mean']:.1f} ‚Üí {patient_data['hr_late_mean']:.1f} bpm (Change: {patient_data['hr_percent_change']:.1f}%)\n",
    "- Systolic BP: {patient_data['bp_early_mean']:.1f} ‚Üí {patient_data['bp_late_mean']:.1f} mmHg (Change: {patient_data['bp_percent_change']:.1f}%)\n",
    "- Respiratory Rate: {patient_data['resp_early_mean']:.1f} ‚Üí {patient_data['resp_late_mean']:.1f} br/min (Change: {patient_data['resp_percent_change']:.1f}%)\n",
    "- Shock Index: {patient_data['shock_index']:.2f} (Normal < 0.7)\n",
    "\n",
    "Provide a brief (3-4 sentence) clinical assessment explaining:\n",
    "1. Whether these vital sign trends support the ML prediction\n",
    "2. The most concerning or reassuring findings\n",
    "3. Clinical significance of the patterns observed\n",
    "\n",
    "Be specific about the physiological implications.\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "def get_llm_explanation(prompt):\n",
    "    \"\"\"\n",
    "    Get explanation from Claude API with updated model\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.messages.create(\n",
    "            model=\"claude-3-5-sonnet-20241022\",  # Updated to newer model\n",
    "            max_tokens=300,\n",
    "            temperature=0.3,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        return response.content[0].text\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Test the API connection\n",
    "print(\"Testing Claude API connection...\")\n",
    "test_response = get_llm_explanation(\"Say 'API working!' if you receive this.\")\n",
    "print(f\"Test response: {test_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad8dd3a-23f9-40a3-962b-04aa5cd9a116",
   "metadata": {},
   "source": [
    "### Verify which model is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4966ad8f-d424-4dd2-a046-649b6c6ba1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying claude-3-5-sonnet-20241022...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\river\\AppData\\Local\\Temp\\ipykernel_56812\\3331381581.py:20: DeprecationWarning: The model 'claude-3-5-sonnet-20241022' is deprecated and will reach end-of-life on October 22, 2025.\n",
      "Please migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.\n",
      "  response = client.messages.create(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå claude-3-5-sonnet-20241022 failed: Error code: 404 - {'type': 'error', 'error': {'type': 'not_found_error', 'message': 'model: claude-3\n",
      "Trying claude-3-opus-20240229...\n",
      "‚ùå claude-3-opus-20240229 failed: Error code: 404 - {'type': 'error', 'error': {'type': 'not_found_error', 'message': 'model: claude-3\n",
      "Trying claude-3-sonnet-20240229...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\river\\AppData\\Local\\Temp\\ipykernel_56812\\3331381581.py:20: DeprecationWarning: The model 'claude-3-opus-20240229' is deprecated and will reach end-of-life on January 5th, 2026.\n",
      "Please migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.\n",
      "  response = client.messages.create(\n",
      "C:\\Users\\river\\AppData\\Local\\Temp\\ipykernel_56812\\3331381581.py:20: DeprecationWarning: The model 'claude-3-sonnet-20240229' is deprecated and will reach end-of-life on July 21st, 2025.\n",
      "Please migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.\n",
      "  response = client.messages.create(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå claude-3-sonnet-20240229 failed: Error code: 404 - {'type': 'error', 'error': {'type': 'not_found_error', 'message': 'model: claude-3\n",
      "Trying claude-3-haiku-20240307...\n",
      "‚úÖ SUCCESS! Model claude-3-haiku-20240307 works!\n",
      "Response: Working\n",
      "\n",
      "ü§î None of the standard models worked. Let me check...\n"
     ]
    }
   ],
   "source": [
    "import anthropic\n",
    "\n",
    "# Your API key is fine - just need correct model name\n",
    "api_key = \"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\"  # Your existing key\n",
    "client = anthropic.Anthropic(api_key=api_key)\n",
    "\n",
    "# Try different model names until we find one that works\n",
    "model_names_to_try = [\n",
    "    \"claude-3-5-sonnet-20241022\",  # Latest Sonnet 3.5\n",
    "    \"claude-3-opus-20240229\",       # Opus\n",
    "    \"claude-3-sonnet-20240229\",     # Older Sonnet (works until July 2025)\n",
    "    \"claude-3-haiku-20240307\",      # Haiku (fast and cheap)\n",
    "]\n",
    "\n",
    "working_model = None\n",
    "\n",
    "for model_name in model_names_to_try:\n",
    "    try:\n",
    "        print(f\"Trying {model_name}...\")\n",
    "        response = client.messages.create(\n",
    "            model=model_name,\n",
    "            max_tokens=50,\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Say 'working'\"}]\n",
    "        )\n",
    "        print(f\"‚úÖ SUCCESS! Model {model_name} works!\")\n",
    "        print(f\"Response: {response.content[0].text}\")\n",
    "        working_model = model_name\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {model_name} failed: {str(e)[:100]}\")\n",
    "\n",
    "if working_model:\n",
    "    print(\"\\nü§î None of the standard models worked. Let me check...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e912454-6372-43ef-b1f7-e99c9f3fd1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Claude Haiku API is working! Generating explanations...\n"
     ]
    }
   ],
   "source": [
    "# Set up with the working model\n",
    "WORKING_MODEL = \"claude-3-haiku-20240307\"\n",
    "client = anthropic.Anthropic(api_key=api_key)\n",
    "\n",
    "def create_clinical_prompt(patient_data, ml_prediction, ml_probability):\n",
    "    \"\"\"Create clinical prompt for Claude\"\"\"\n",
    "    prompt = f\"\"\"You are an ICU physician reviewing patient vital signs and ML predictions.\n",
    "\n",
    "Patient: {patient_data['segment']}\n",
    "Duration: {patient_data['duration_hours']:.1f} hours\n",
    "ML Prediction: {'DETERIORATING' if ml_prediction == 1 else 'STABLE'}\n",
    "Risk: {ml_probability:.1%}\n",
    "\n",
    "Vital Changes:\n",
    "- Heart Rate: {patient_data['hr_early_mean']:.1f} ‚Üí {patient_data['hr_late_mean']:.1f} bpm ({patient_data['hr_percent_change']:.1f}%)\n",
    "- Systolic BP: {patient_data['bp_early_mean']:.1f} ‚Üí {patient_data['bp_late_mean']:.1f} mmHg ({patient_data['bp_percent_change']:.1f}%)\n",
    "- Respiratory: {patient_data['resp_early_mean']:.1f} ‚Üí {patient_data['resp_late_mean']:.1f} br/min ({patient_data['resp_percent_change']:.1f}%)\n",
    "- Shock Index: {patient_data['shock_index']:.2f} (Normal < 0.7)\n",
    "\n",
    "Provide a 3-4 sentence clinical assessment of whether the vital signs support the ML prediction and the clinical significance.\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def get_llm_explanation(prompt):\n",
    "    \"\"\"Get explanation from Claude Haiku\"\"\"\n",
    "    try:\n",
    "        response = client.messages.create(\n",
    "            model=WORKING_MODEL,\n",
    "            max_tokens=300,\n",
    "            temperature=0.3,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        return response.content[0].text\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "print(\"‚úÖ Claude Haiku API is working! Generating explanations...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0bdede2f-5bc7-499b-b23d-d2fd8de8e4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Generating explanations using claude-3-haiku-20240307\n",
      "Cost estimate: ~$0.05 for all 91 patients (Haiku is 10x cheaper!)\n",
      "\n",
      "‚úÖ Processed 20/91...\n",
      "‚úÖ Processed 40/91...\n",
      "‚úÖ Processed 60/91...\n",
      "‚úÖ Processed 80/91...\n",
      "\n",
      "‚úÖ Generated 91/91 explanations successfully!\n",
      "üíæ Saved to: FINAL_WITH_HAIKU_EXPLANATIONS.csv\n",
      "\n",
      "üìã Example Test Patient:\n",
      "Patient: 30/3000393\n",
      "Prediction: STABLE (23.0%)\n",
      "Claude says: The vital sign changes observed in this patient do not fully support the ML prediction of STABLE. The increase in heart rate by 15.6% and the normal Shock Index suggest some hemodynamic instability, which may indicate the need for closer monitoring. However, the relatively small changes in systolic ...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "print(f\"üöÄ Generating explanations using {WORKING_MODEL}\")\n",
    "print(\"Cost estimate: ~$0.05 for all 91 segments (Haiku is 10x cheaper!)\\n\")\n",
    "\n",
    "# Get corrected model predictions\n",
    "all_predictions = model_fixed.predict(df[feature_columns].values)\n",
    "all_probabilities = model_fixed.predict_proba(df[feature_columns].values)[:, 1]\n",
    "\n",
    "corrected_explanations = []\n",
    "successful = 0\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    pred = all_predictions[idx]\n",
    "    prob = all_probabilities[idx]\n",
    "    \n",
    "    prompt = create_clinical_prompt(row, pred, prob)\n",
    "    explanation = get_llm_explanation(prompt)\n",
    "    \n",
    "    if not explanation.startswith(\"Error\"):\n",
    "        successful += 1\n",
    "    \n",
    "    is_test = row['patient_id'] in test_patients\n",
    "    \n",
    "    corrected_explanations.append({\n",
    "        'patient_id': row['patient_id'],\n",
    "        'segment': row['segment'],\n",
    "        'duration_hours': row['duration_hours'],\n",
    "        'hr_change': row['hr_percent_change'],\n",
    "        'bp_change': row['bp_percent_change'],\n",
    "        'resp_change': row['resp_percent_change'],\n",
    "        'shock_index': row['shock_index'],\n",
    "        'ml_prediction': 'DETERIORATING' if pred == 1 else 'STABLE',\n",
    "        'ml_probability': prob,\n",
    "        'claude_explanation': explanation,\n",
    "        'is_test_set': is_test\n",
    "    })\n",
    "    \n",
    "    if (idx + 1) % 20 == 0:\n",
    "        print(f\"‚úÖ Processed {idx + 1}/91...\")\n",
    "        time.sleep(0.2)  # Small delay for rate limiting\n",
    "\n",
    "# Save results\n",
    "final_df = pd.DataFrame(corrected_explanations)\n",
    "final_df.to_csv('FINAL_WITH_HAIKU_EXPLANATIONS.csv', index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Generated {successful}/91 explanations successfully!\")\n",
    "print(f\"üíæ Saved to: FINAL_WITH_HAIKU_EXPLANATIONS.csv\")\n",
    "\n",
    "# Show example\n",
    "if successful > 0:\n",
    "    ex = final_df[final_df['is_test_set'] == True].iloc[0]\n",
    "    print(f\"\\nüìã Example Test Patient:\")\n",
    "    print(f\"Patient: {ex['patient_id']}\")\n",
    "    print(f\"Prediction: {ex['ml_prediction']} ({ex['ml_probability']:.1%})\")\n",
    "    print(f\"Claude says: {ex['claude_explanation'][:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8f2f0d-eeac-485a-9d43-95c8ec4a5066",
   "metadata": {},
   "source": [
    "# Step 33: Interactive Dashboard using Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "37de36a3-4cb0-4131-ae95-e14decb2672d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: streamlit in c:\\users\\river\\anaconda3\\lib\\site-packages (1.37.1)\n",
      "Requirement already satisfied: plotly in c:\\users\\river\\anaconda3\\lib\\site-packages (5.24.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\river\\anaconda3\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\river\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: altair<6,>=4.0 in c:\\users\\river\\anaconda3\\lib\\site-packages (from streamlit) (5.0.1)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in c:\\users\\river\\anaconda3\\lib\\site-packages (from streamlit) (1.6.2)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in c:\\users\\river\\anaconda3\\lib\\site-packages (from streamlit) (5.3.3)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\river\\anaconda3\\lib\\site-packages (from streamlit) (8.1.7)\n",
      "Requirement already satisfied: packaging<25,>=20 in c:\\users\\river\\anaconda3\\lib\\site-packages (from streamlit) (24.0)\n",
      "Requirement already satisfied: pillow<11,>=7.1.0 in c:\\users\\river\\anaconda3\\lib\\site-packages (from streamlit) (10.4.0)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in c:\\users\\river\\anaconda3\\lib\\site-packages (from streamlit) (4.25.3)\n",
      "Requirement already satisfied: pyarrow>=7.0 in c:\\users\\river\\anaconda3\\lib\\site-packages (from streamlit) (16.1.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\river\\anaconda3\\lib\\site-packages (from streamlit) (2.32.3)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in c:\\users\\river\\anaconda3\\lib\\site-packages (from streamlit) (13.7.1)\n",
      "Requirement already satisfied: tenacity<9,>=8.1.0 in c:\\users\\river\\anaconda3\\lib\\site-packages (from streamlit) (8.2.3)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in c:\\users\\river\\anaconda3\\lib\\site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in c:\\users\\river\\anaconda3\\lib\\site-packages (from streamlit) (4.11.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in c:\\users\\river\\anaconda3\\lib\\site-packages (from streamlit) (3.1.43)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in c:\\users\\river\\anaconda3\\lib\\site-packages (from streamlit) (0.8.0)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\river\\anaconda3\\lib\\site-packages (from streamlit) (6.4.1)\n",
      "Requirement already satisfied: watchdog<5,>=2.1.5 in c:\\users\\river\\anaconda3\\lib\\site-packages (from streamlit) (4.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\river\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\river\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\river\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\river\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\river\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
      "Requirement already satisfied: toolz in c:\\users\\river\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (0.12.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\river\\anaconda3\\lib\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\river\\anaconda3\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.7)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\river\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\river\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\river\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\river\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\river\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2025.4.26)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\river\\anaconda3\\lib\\site-packages (from rich<14,>=10.14.0->streamlit) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\river\\anaconda3\\lib\\site-packages (from rich<14,>=10.14.0->streamlit) (2.15.1)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in c:\\users\\river\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\river\\anaconda3\\lib\\site-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\river\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\river\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\river\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\river\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.10.6)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\river\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install streamlit plotly pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "293277d2-5195-4e15-b7a6-2944fa3706a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 00:11:50.012 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\river\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-11-06 00:11:50.012 No runtime found, using MemoryCacheStorageManager\n",
      "2025-11-06 00:11:50.014 No runtime found, using MemoryCacheStorageManager\n",
      "2025-11-06 00:11:50.397 Session state does not function when running a script without `streamlit run`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeltaGenerator()"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import anthropic\n",
    "\n",
    "# Page configuration\n",
    "st.set_page_config(\n",
    "    page_title=\"ICU Deterioration Detection System\",\n",
    "    page_icon=\"üè•\",\n",
    "    layout=\"wide\",\n",
    "    initial_sidebar_state=\"expanded\"\n",
    ")\n",
    "\n",
    "# Custom CSS for better styling\n",
    "st.markdown(\"\"\"\n",
    "    <style>\n",
    "    .main {padding: 0rem 0rem;}\n",
    "    .stMetric {background-color: #f0f2f6; padding: 10px; border-radius: 5px;}\n",
    "    </style>\n",
    "    \"\"\", unsafe_allow_html=True)\n",
    "\n",
    "# Load your data\n",
    "@st.cache_data\n",
    "def load_data():\n",
    "    return pd.read_csv('FINAL_WITH_HAIKU_EXPLANATIONS.csv')\n",
    "\n",
    "df = load_data()\n",
    "\n",
    "# Initialize Claude (for live queries)\n",
    "api_key = \"sk-ant-api03-...\"  # Your API key here\n",
    "client = anthropic.Anthropic(api_key=api_key)\n",
    "\n",
    "# Header\n",
    "st.title(\"üè• ICU Patient Deterioration Detection System\")\n",
    "st.markdown(\"**AI-Powered Early Warning System** | MIMIC-III Waveform Database Analysis\")\n",
    "\n",
    "# Sidebar\n",
    "st.sidebar.image(\"https://via.placeholder.com/300x100/4A90E2/FFFFFF?text=ICU+Monitor\", use_column_width=True)\n",
    "st.sidebar.markdown(\"---\")\n",
    "st.sidebar.header(\"üìä Control Panel\")\n",
    "\n",
    "# Mode selection\n",
    "mode = st.sidebar.selectbox(\n",
    "    \"Select Mode:\",\n",
    "    [\"üë• Patient Database\", \"üéÆ Live Simulation\", \"üìà Analytics Overview\"]\n",
    ")\n",
    "\n",
    "if mode == \"üë• Patient Database\":\n",
    "    # Patient selection\n",
    "    patients = df['patient_id'].unique()\n",
    "    selected_patient = st.sidebar.selectbox(\n",
    "        \"Select Patient ID:\",\n",
    "        patients,\n",
    "        format_func=lambda x: f\"Patient {x}\"\n",
    "    )\n",
    "    \n",
    "    # Get patient's segments\n",
    "    patient_segments = df[df['patient_id'] == selected_patient]\n",
    "    if len(patient_segments) > 1:\n",
    "        segment_idx = st.sidebar.selectbox(\n",
    "            \"Select Recording Segment:\",\n",
    "            range(len(patient_segments)),\n",
    "            format_func=lambda x: f\"Segment {x+1} - {patient_segments.iloc[x]['segment']}\"\n",
    "        )\n",
    "        patient_data = patient_segments.iloc[segment_idx]\n",
    "    else:\n",
    "        patient_data = patient_segments.iloc[0]\n",
    "    \n",
    "    # Main dashboard layout\n",
    "    st.markdown(\"---\")\n",
    "    \n",
    "    # Top metrics row\n",
    "    col1, col2, col3, col4, col5 = st.columns(5)\n",
    "    \n",
    "    with col1:\n",
    "        status_color = \"üî¥\" if patient_data['ml_prediction'] == \"DETERIORATING\" else \"üü¢\"\n",
    "        st.metric(\n",
    "            label=\"Status\",\n",
    "            value=patient_data['ml_prediction'],\n",
    "            delta=f\"{status_color} Risk: {patient_data['ml_probability']:.1%}\"\n",
    "        )\n",
    "    \n",
    "    with col2:\n",
    "        hr_icon = \"‚¨ÜÔ∏è\" if patient_data['hr_change'] > 10 else \"‚¨áÔ∏è\" if patient_data['hr_change'] < -10 else \"‚û°Ô∏è\"\n",
    "        st.metric(\n",
    "            label=\"üíì Heart Rate\",\n",
    "            value=f\"{patient_data['hr_change']:.1f}%\",\n",
    "            delta=hr_icon\n",
    "        )\n",
    "    \n",
    "    with col3:\n",
    "        bp_icon = \"‚¨ÜÔ∏è\" if patient_data['bp_change'] > 10 else \"‚¨áÔ∏è\" if patient_data['bp_change'] < -10 else \"‚û°Ô∏è\"\n",
    "        st.metric(\n",
    "            label=\"ü©∫ Blood Pressure\",\n",
    "            value=f\"{patient_data['bp_change']:.1f}%\",\n",
    "            delta=bp_icon\n",
    "        )\n",
    "    \n",
    "    with col4:\n",
    "        resp_icon = \"‚¨ÜÔ∏è\" if patient_data['resp_change'] > 15 else \"‚¨áÔ∏è\" if patient_data['resp_change'] < -15 else \"‚û°Ô∏è\"\n",
    "        st.metric(\n",
    "            label=\"ü´Å Respiratory\",\n",
    "            value=f\"{patient_data['resp_change']:.1f}%\",\n",
    "            delta=resp_icon\n",
    "        )\n",
    "    \n",
    "    with col5:\n",
    "        shock_status = \"‚ö†Ô∏è High\" if patient_data['shock_index'] > 0.7 else \"‚úÖ Normal\"\n",
    "        st.metric(\n",
    "            label=\"‚ö° Shock Index\",\n",
    "            value=f\"{patient_data['shock_index']:.2f}\",\n",
    "            delta=shock_status\n",
    "        )\n",
    "    \n",
    "    st.markdown(\"---\")\n",
    "    \n",
    "    # Risk visualization and AI analysis\n",
    "    col1, col2 = st.columns([1, 2])\n",
    "    \n",
    "    with col1:\n",
    "        st.subheader(\"‚öïÔ∏è Risk Assessment\")\n",
    "        \n",
    "        # Risk gauge\n",
    "        fig = go.Figure(go.Indicator(\n",
    "            mode = \"gauge+number+delta\",\n",
    "            value = patient_data['ml_probability'] * 100,\n",
    "            domain = {'x': [0, 1], 'y': [0, 1]},\n",
    "            title = {'text': \"Deterioration Risk\", 'font': {'size': 20}},\n",
    "            delta = {'reference': 50, 'increasing': {'color': \"red\"}},\n",
    "            gauge = {\n",
    "                'axis': {'range': [None, 100], 'tickwidth': 1, 'tickcolor': \"darkblue\"},\n",
    "                'bar': {'color': \"darkred\" if patient_data['ml_probability'] > 0.5 else \"darkgreen\"},\n",
    "                'bgcolor': \"white\",\n",
    "                'borderwidth': 2,\n",
    "                'bordercolor': \"gray\",\n",
    "                'steps': [\n",
    "                    {'range': [0, 25], 'color': '#90EE90'},\n",
    "                    {'range': [25, 50], 'color': '#FFFFE0'},\n",
    "                    {'range': [50, 75], 'color': '#FFD700'},\n",
    "                    {'range': [75, 100], 'color': '#FF6B6B'}\n",
    "                ],\n",
    "                'threshold': {\n",
    "                    'line': {'color': \"red\", 'width': 4},\n",
    "                    'thickness': 0.75,\n",
    "                    'value': 90\n",
    "                }\n",
    "            }\n",
    "        ))\n",
    "        fig.update_layout(height=300, font={'color': \"darkblue\", 'family': \"Arial\"})\n",
    "        st.plotly_chart(fig, use_container_width=True)\n",
    "        \n",
    "        # Test/Train indicator\n",
    "        if patient_data['is_test_set']:\n",
    "            st.info(\"üìù This patient was in the TEST set\")\n",
    "        else:\n",
    "            st.success(\"üìö This patient was in the TRAINING set\")\n",
    "    \n",
    "    with col2:\n",
    "        st.subheader(\"ü§ñ AI Clinical Assessment\")\n",
    "        \n",
    "        # Tabs for different views\n",
    "        tab1, tab2, tab3 = st.tabs([\"üí¨ Analysis\", \"üîÑ Get Fresh Opinion\", \"üìä Details\"])\n",
    "        \n",
    "        with tab1:\n",
    "            st.info(patient_data['claude_explanation'])\n",
    "        \n",
    "        with tab2:\n",
    "            if st.button(\"üîÆ Get New AI Analysis\", type=\"primary\"):\n",
    "                with st.spinner(\"Consulting AI physician...\"):\n",
    "                    prompt = f\"\"\"As an ICU physician, provide a brief (3 sentence) assessment of:\n",
    "                    Patient with {patient_data['hr_change']:.1f}% HR change, \n",
    "                    {patient_data['bp_change']:.1f}% BP change, \n",
    "                    {patient_data['resp_change']:.1f}% respiratory change.\n",
    "                    Shock index: {patient_data['shock_index']:.2f}\n",
    "                    ML predicts {patient_data['ml_probability']:.1%} deterioration risk.\"\"\"\n",
    "                    \n",
    "                    response = client.messages.create(\n",
    "                        model=\"claude-3-haiku-20240307\",\n",
    "                        max_tokens=200,\n",
    "                        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "                    )\n",
    "                    st.success(response.content[0].text)\n",
    "        \n",
    "        with tab3:\n",
    "            st.write(f\"**Recording Duration:** {patient_data['duration_hours']:.1f} hours\")\n",
    "            st.write(f\"**Patient ID:** {patient_data['patient_id']}\")\n",
    "            st.write(f\"**Segment:** {patient_data['segment']}\")\n",
    "            st.write(f\"**Model Confidence:** {patient_data['ml_probability']:.1%}\")\n",
    "\n",
    "elif mode == \"üéÆ Live Simulation\":\n",
    "    st.subheader(\"üéÆ Interactive Patient Simulator\")\n",
    "    st.markdown(\"Adjust vital signs to see real-time risk assessment\")\n",
    "    \n",
    "    col1, col2 = st.columns(2)\n",
    "    \n",
    "    with col1:\n",
    "        st.markdown(\"### üìä Vital Sign Changes\")\n",
    "        hr_change = st.slider(\"Heart Rate Change (%)\", -30, 30, 0, 1)\n",
    "        bp_change = st.slider(\"Blood Pressure Change (%)\", -30, 30, 0, 1)\n",
    "        resp_change = st.slider(\"Respiratory Change (%)\", -30, 30, 0, 1)\n",
    "        shock_index = st.slider(\"Shock Index\", 0.3, 1.2, 0.6, 0.01)\n",
    "        \n",
    "        if st.button(\"üî¨ Analyze Patient\", type=\"primary\"):\n",
    "            # Simple risk calculation based on thresholds\n",
    "            risk_score = 0\n",
    "            if abs(hr_change) > 10: risk_score += 0.3\n",
    "            if bp_change < -10: risk_score += 0.3\n",
    "            if resp_change > 15: risk_score += 0.3\n",
    "            if shock_index > 0.7: risk_score += 0.1\n",
    "            \n",
    "            risk_score = min(risk_score, 0.99)\n",
    "            \n",
    "            with col2:\n",
    "                st.markdown(\"### üè• Analysis Results\")\n",
    "                \n",
    "                if risk_score > 0.5:\n",
    "                    st.error(f\"‚ö†Ô∏è HIGH RISK - Deterioration likely ({risk_score:.1%})\")\n",
    "                else:\n",
    "                    st.success(f\"‚úÖ LOW RISK - Stable patient ({risk_score:.1%})\")\n",
    "                \n",
    "                # Get AI explanation\n",
    "                with st.spinner(\"Getting AI assessment...\"):\n",
    "                    prompt = f\"\"\"Brief 2-sentence assessment: Patient with HR change {hr_change}%, \n",
    "                    BP change {bp_change}%, Resp change {resp_change}%, Shock index {shock_index:.2f}\"\"\"\n",
    "                    \n",
    "                    response = client.messages.create(\n",
    "                        model=\"claude-3-haiku-20240307\",\n",
    "                        max_tokens=100,\n",
    "                        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "                    )\n",
    "                    st.info(response.content[0].text)\n",
    "\n",
    "else:  # Analytics Overview\n",
    "    st.subheader(\"üìà Model Performance Analytics\")\n",
    "    \n",
    "    col1, col2, col3 = st.columns(3)\n",
    "    \n",
    "    with col1:\n",
    "        total_patients = df['patient_id'].nunique()\n",
    "        st.metric(\"Total Patients\", total_patients)\n",
    "    \n",
    "    with col2:\n",
    "        high_risk = (df['ml_probability'] > 0.5).sum()\n",
    "        st.metric(\"High Risk Cases\", f\"{high_risk}/{len(df)}\")\n",
    "    \n",
    "    with col3:\n",
    "        avg_risk = df['ml_probability'].mean()\n",
    "        st.metric(\"Average Risk\", f\"{avg_risk:.1%}\")\n",
    "    \n",
    "    # Risk distribution\n",
    "    fig = px.histogram(df, x='ml_probability', nbins=20, \n",
    "                       title=\"Risk Score Distribution\",\n",
    "                       labels={'ml_probability': 'Deterioration Risk', 'count': 'Number of Patients'})\n",
    "    st.plotly_chart(fig, use_container_width=True)\n",
    "\n",
    "# Footer with chat\n",
    "st.markdown(\"---\")\n",
    "st.markdown(\"### üí¨ Ask the AI Physician\")\n",
    "user_question = st.text_input(\"Ask any question about ICU patient monitoring:\")\n",
    "\n",
    "if user_question:\n",
    "    with st.spinner(\"Thinking...\"):\n",
    "        response = client.messages.create(\n",
    "            model=\"claude-3-haiku-20240307\",\n",
    "            max_tokens=200,\n",
    "            messages=[{\"role\": \"user\", \"content\": f\"As an ICU physician, answer: {user_question}\"}]\n",
    "        )\n",
    "        st.write(f\"**AI Physician:** {response.content[0].text}\")\n",
    "\n",
    "# Info footer\n",
    "st.markdown(\"---\")\n",
    "st.caption(\"üè• ICU Deterioration Detection System | Powered by ML + Claude AI | MIMIC-III Database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0c1977-efc1-4f9b-9dda-47cafed33664",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
